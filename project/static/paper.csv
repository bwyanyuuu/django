Session ID,Paper Title,Author,School,Content
Oral 1,Global Relation-Aware Attention Network for Image-Text Retrieval,"Jie Cao, Shengsheng Qian, Huaiwen Zhang, Quan Fang and Changsheng Xu","School of Artificial Intelligence, University of Chinese Academy of Sciences, National Lab of Pattern Recognition Institute of Automation, Chinese Academy of Sciences","The cross-modal image-text retrieval has attracted extensive attention in recent years, which contributes to the development of search engine. Fine-grained features and cross-attention have been widely used in past researches to reach the goal of cross-modal image-text matching. Although cross-related methods have achieved remarkable results, the features must be encoded again in evaluation phase due to the interaction of the two modalities, which is unsuitable for actual scenarios of search engine development. In addition, the aggregated feature does not contain sufficient semantics since it is merely obtained by simple mean pooling. Furthermore, connecting weights of self-attention blocks are target position invariant, which lacks the expected adaptability. To tackle these limitations, in this paper, we propose a novel Relation-aware Global Attention Network (RGAN) for image-text retrieval by designing Global Attention Module (GAM) and Relation-aware Attention Module (RAM) which play an important role in modeling the global feature and the relationships of local fragments. Firstly, we propose Global Attention Module (GAM) followed the fine-grained features to obtain meaningful global feature. Secondly, we use several stacked transformer encoders to further encode features separately. Finally, we propose Relation-aware Attention Module (RAM) to generate a vector which represents the relation information to infer the attention intensity of pairwise fragments. The local features, the global feature, and their relations are considered jointly to conduct an efficient image-text retrieval. Extensive experiments are conducted on the benchmark datasets of Flickr30K and MSCOCO, demonstrating the superiority of our method. On the Flickr30K, compared to the state-of-the-art method TERAN, we improve Recall@K(K=1) metric by 5.8% and 4.0% on the image and text retrieval tasks, respectively."
Oral 1,Multi-Feature Graph Attention Network for Cross-Modal Video-Text Retrieval,"Xiaoshuai Hao, Yucan Zhou, Dayan Wu, Wanqian Zhang, Bo Li and Weiping Wang","of Information Engineering,Chinese Academy of Sciences & University of Chinese Academy of Sciences, Institute of Information Engineering, Chinese Academy of Sciences, Institute of Information Engineering, Chinese Academy of Sciences, Institute of Information Engineering, Chinese Academy of Sciences, Institute of Information Engineering, Chinese Academy of Sciences","Cross-modal retrieval between videos and texts has attracted growing attention due to the rapid growth of user-generated videos on the web. To solve this problem most approaches try to learn a joint embedding space to measure the cross-modal similarities, while paying little attention to the representation of each modality. Video is more complicated than the commonly used visual feature, since the audio and caption on the screen also contain rich information.Recently, the aggregations of multiple features in videos boost the benchmark of the video-text retrieval system.  However, they usually handle each feature independently, which ignores the interchange of high-level semantic relations among these multiple features. Moreover, despite the inter-modal ranking constraint where semantically-similar texts and videos should stay closer, the modality-specific requirement, i.e. two similar videos/texts should have similar representations, is also significant. In this paper, we propose a novel Multi-Feature Graph ATtention Network (MFGATN) for cross-modal video-text retrieval.Specifically, we introduce a multi-feature graph attention module, which enriches the representation of each feature in videos with the interchange of high-level semantic information among them.Moreover, we elaborately design a novel Dual Constraint Ranking Loss (DCRL), which simultaneously considers the inter-modal ranking constraint and the intra-modal structure constraint to preserve both the cross-modal semantic similarity and the modality-specific consistency in the embedding space.Experiments on two datasets, i.e. MSR-VTT and MSVD, demonstrate that our method achieves significant performance gain compared with the state-of-the-arts."
Oral 1,TEACH: Attention-Aware Deep Cross-Modal Hashing,"Hong-Lei Yao, Yu-Wei Zhan, Zhen-Duo Chen, Xin Luo and Xin-Shun Xu","Shandong University, Shandong University, Shandong University, Shandong University","Hashing methods for cross-modal retrieval have recently been widely investigated due to the explosive growth of multimedia data. Generally, real-world data is imperfect and has more or less redundancy, making cross-modal retrieval task challenging. However, most existing cross-modal hashing methods fail to deal with the redundancy, leading to unsatisfactory performance on such data. In this paper, to address this issue, we propose a novel cross-modal hashing method, namely aTtEntion-Aware deep Cross-modal Hashing (TEACH). It could perform feature learning and hash-code learning simultaneously. Besides, with designed attention modules for different modalities, one for each, TEACH can effectively highlight the useful information of data while suppressing the redundant information. Extensive experiments on benchmark datasets demonstrate that our method outperforms some state-of-the-art hashing methods in cross-modal retrieval tasks."
Oral 1,HSGMP: Heterogeneous Scene Graph Message Passing for Cross-modal Retrieval,"Yu Duan, Yun Xiong, Yao Zhang, Yuwei Fu and Yangyong Zhu","Fudan University, Fudan University Fudan University, Fudan University, Fudan University & Shanghai Institute for Advanced Communication and Data Science","Semantic relationship information is important to the image-text retrieval task. Existing work usually extract relationship information by calculating the relationship value pairwise, which is hardly to find out a meaningful semantic relationship. A more reasonable method is to convert the modal to a scene graph, thereby explicitly modeling the relationship. Scene graph is a kind of graph data structure modeling the scene of modality. There are two concept in a scene graph, object and relationship. In image modal, object indicates the image region and relationship represents the predicate of the image regions. In text modal, object indicates the entity and relationship represents the association between entities, also known as semantic relationship. In image-text retrieval task, both object and relationship are important, and a key challenge is to obtain semantic information. In this paper, image and text are represented as two kinds of scene graphs: visual scene graph and textual scene graph, and then they are combined into Heterogeneous Scene Graph(HSG). By explicitly modeling relationships using directed graph, the information can be passed edge-wise. To further extract semantic information, we introduce the metapath, which can extract specific semantic information on specified path. Moreover, we propose Heterogeneous Message Passing(HMP) to communicate information on the metapath.After the message passing, the similarity of two modalities can be represented as the similarity of the graphs.Experiment shows that the model achieve competitive results on Flickr30K and MSCOCO, which indicates that our approach has advantages in image-text retrieval."
Oral 1,Unsupervised Deep Cross-Modal Hashing by Knowledge Distillation for Large-scale Cross-modal Retrieval,Mingyong Li and Hongya Wang,"Donghua University & Chongqing Normal University, Donghua University & Shanghai Key Laboratory of Computer Software Evaluating and Testing","Cross-modal hashing (CMH) maps heterogeneous multiple modality data into compact binary code to achieve fast and flexible retrieval across different modalities, especially in large-scale retrieval. As the data don't need a lot of manual annotation, unsupervised cross-modal hashing has a wider application prospect than supervised method. However, the existing unsupervised methods are difficult to achieve satisfactory performance due to the lack of credible supervisory information. To solve this problem, inspired by knowledge distillation, we propose a novel unsupervised Knowledge Distillation Cross-Modal Hashing method (KDCMH), which can use similarity information distilled from unsupervised method to guide supervised method. Specifically, firstly, the teacher model adopted an unsupervised distribution-based similarity hashing method, which can construct a modal fusion similarity matrix.Secondly, under the supervision of teacher model distillation information, student model can generate more discriminative hash codes. In two public datasets NUS-WIDE and MIRFLICKR25K, extensive experiments have proved the significant improvement of KDCMH on several representative unsupervised cross-modal hashing methods."
Challenge 1,"Radar Object Detection Using Data Merging, Enhancement and Fusion","Jun Yu, Xinlong Hao, Xinjian Gao, Qiang Sun, Yuyu Liu, Peng Chang, Zhong Zhang, Fang Gao and Feng Shuang","University of Science and Technology of China,  Ping An Technology Co., Ltd,  PAII labs, Palo Alto,  Hefei ZhanDa Intelligence Technology Co., Ltd and  Guangxi University","Compared to visible images, radar images are generally considered to be an active and robust solution, even in adverse driving situations, for object detection. However, the accuracy of radar object detection (ROD) is always poor. Owing to taking full advantage of data merging, enhancement and fusion, this paper proposes an effective ROD system with only radar images as the input. First, an aggregation module is designed to merge the data from all chirps in the same frame. Then, various gaussian noises with different parameters are employed to increase data diversity and reduce over-fitting based on the analysis of training data. Moreover, due to the process of inference with default parameters is not accurate enough, some hyperparameters are changed to increase the accuracy performance. Finally, a combination strategy is adopted to benefit from multi-model fusion. ROD2021 Challenge is supported by ACM ICMR 2021, and our team (ustc-nelslip) ranked 2nd in the test stage of this challenge. Diverse evaluations also verify the superiority of the proposed system. The code is available at https://github.com/haoxinl/-ICMR_2021_ROD2021_Challenge_2nd_place_solution_ustc-nelslip"
Challenge 1,Squeeze-and-Excitation network-Based Radar Object Detection With Weighted Location Fusion,"Pengliang Sun, Xuetong Niu, Pengfei Sun and Kele Xu","The Chinese University of Hong Kong, King’s College London, Institute of Neuroinformatics, University of Zurich and ETH Zurich, National Key Laboratory of Parallel and Distributed Processing","Radar object detection refers to identify objects from the radar data, and the topic has received increasing interest during last years, due to the appealing property of radar imaging and evident applications. However, the detection performance heavily relied on the semantic information extraction, which is of a great challenge in the practical settings. Moreover, despite remarkable progress have been made, most of the previous attempts are restrained from the essentially limited property of the employed single modality. Inspired by the recent success of cross-modality deep learning, we propose a novel cross-modality deep learning framework for the radar object detection task using the Squeeze-and-Excitation network, which aims to provide more powerful feature representation. Moreover, a novel noisy detection approach is also explored in our study, to increase the model's ability to handle with noise. Finally, a novel weighted location fusion strategy is introduced in our framework, with the goal to improve the detection performance further. To empirically investigate the effectiveness of the proposed framework, we conduct extensive experiments on the 2021 ICMR ROD challenge. The obtained results suggest that the proposed framework outperforms related approaches. Our method ranks as the 5th place on the final leaderboard, with an average precision (AP) percentage of 76.1. Models and codes are available at https://github.com/sunpengliang/modelConfusion. "
Challenge 1,Scene-aware Learning Network for Radar Object Detection,"Zangwei Zheng, Xiangyu Yue, Kurt Keutzer and Alberto Sangiovanni Vincentelli","Nanjing University, University of California, Berkeley, University of California, Berkeley, University of California, Berkeley","Object detection is essential to safe autonomous or assisted driving. Previous works usually utilize RGB images or LiDAR point clouds to identify and localize multiple objects in self-driving. However, cameras tend to fail in bad driving conditions, e.g. bad weather or weak lighting, while LiDAR scanners are too expensive to get widely deployed in commercial applications. Radar has been drawing more and more attention due to its robustness and low cost. In this paper, we propose a scene-aware radar learning framework for accurate and robust object detection. First, the learning framework contains branches conditioning on the scene category of the radar sequence; with each branch optimized for a specific type of scene. Second, three different 3D autoencoder-based architectures are proposed for radar object detection and ensemble learning is performed over the different architectures to further boost the final performance. Third, we propose novel scene-aware sequence mix augmentation (SceneMix) and scene-specific post-processing to generate more robust detection results. In the ROD2021 Challenge, we achieved a final result of average precision of 75.0% and an average recall of 81.0%. Moreover, in the parking lot scene, our framework ranks first with an average precision of 97.8% and an average recall of 98.6%, which demonstrates the effectiveness of our framework."
Challenge 1,DANet: Dimension Apart Network for Radar Object Detection,"Bo Ju, Wei Yang, Jinrang Jia, Xiaoqing Ye, Qu Chen, Xiao Tan, Hao Sun, Yifeng Shi and Errui Ding","Baidu Inc., Baidu Inc., Baidu Inc., Baidu Inc.,Baidu Inc.,Baidu Inc.,Baidu Inc.,Baidu Inc.","In this paper, we propose a dimension apart network (DANet) for radar object detection task. A Dimension Apart Module (DAM) is first designed to be lightweight and capable of extracting temporal-spatial information from the RAMap sequences. To fully utilize the hierarchical features from the RAMaps, we propose a multi-scale U-Net style network architecture termed DANet. Extensive experiments demonstrate that our proposed DANet achieves superior performance on the radar detection task at much less computational cost, compared to previous pioneer works. In addition to the proposed novel network, we also utilize a vast amount of data augmentation techniques. To further improve the robustness of our model, we ensemble the predicted results from a bunch of lightweight DANet variants. Finally, we achieve 82.2% on average precision and 90% on average recall of object detection performance and rank at 1st place in the ROD2021 radar detection challenge. Here is the GitHub link to our code: https://github.com/jb892/ROD2021_Radar_Detection_Challenge_Baidu"
Challenge 1,Efficient-ROD: Efficient Radar Object Detection based on Densely Connected Residual Network,"Chih-Chung Hsu, Chieh Lee, Lin Chen, Min-Kai Hung, Yu-Lun Chen and Xian-Yu Wang","National Cheng Kung University, National Cheng Kung University, National Pingtung University of science and technology, National Pingtung University of Science and Technology, National Cheng Kung University","Radar signal-based object detection has become a primary and critical issue for autonomous driving recently. However, the trade-off between computational efficiency and radar object detection performance is still not well addressed. This paper proposes a lightweight, computationally efficient, and effective network architecture to conquer this issue. First, a densely connected residual block (DCSB) is proposed to better deliver the gradient flow from the loss function to improve the feature representational power. On the other hand, to keep the spatial resolution with a larger receptive field in the backbone network, Atrous convolution greatly reduces the parameters and keeps the same receptive field as the conventional convolution with a large kernel size. Finally, the proposed DCSB-based Network, termed DCSN, is composed by stacking three DCSB blocks from an encoder and adopting three transposed convolution layers to build the decoder. The Mish activation function is used on the proposed DCSN to accelerate the training process to be converged. In this manner, we can keep a larger receptive field as well as reduce the number of parameters significantly, resulting in computational efficiency radar object detection. Experiments is demonstrated that the proposed DCSN achieves a significant improvement of inference time and computational complexity."
Challenge 1,ROD2021 Challenge: A Summary for Radar Object Detection Challenge for Autonomous Driving Applications,"Yizhou Wang, Jenq-Neng Hwang, Gaoang Wang, Hui Liu, Kwang-Ju Kim, Hung-Min Hsu, Jiarui Cai, Haotian Zhang, Zhongyu Jiang and Renshu Gu.","University of Washington, University of Washington, Zhejiang University, University of Washington & Silkwave Holdings Limited, Electronics and Telecommunications Research Institute (ETRI), University of Washington, University of Washington, University of Washington, University of Washington, Hangzhou Dianzi University","The Radar Object Detection 2021 (ROD2021) Challenge, held in the ACM International Conference on Multimedia Retrieval (ICMR) 2021, has been introduced to detect and classify objects purely using an FMCW radar for autonomous driving applications. As a robust sensor to all-weather conditions, radar has rich information hidden in the radio frequencies, which can potentially achieve object detection and classification. This insight will provide a new object perception solution for an autonomous vehicle even in adverse driving scenarios. The ROD2021 Challenge is the first public benchmark focusing on this topic, which attracts great attention and participation. There are more than 260 participants among 37 teams from more than 10 countries with different academic and industrial affiliations, contributing about 300 submissions in the first phase and 400 submissions in the second phase. The final performance is evaluated by average precision (AP). Results add strong value and a better understanding of the radar object detection task for the autonomous vehicle community. "
Oral 2,Image-to-Image Transfer Makes Chaos to Order,"Sanbi Luo, Tao Guo","Institute of Information Engineering, Chinese Academy of Sciences & University of Chinese Academy of Sciences, Institute of Information Engineering, Chinese Academy of Sciences & University of Chinese Academy of Sciences","GAN-based image-to-image transfer has achieved remarkable results in image generation. However, most of the researches focus on changing the style of images. The contents, especially the object locations between input and output images, always keep consistent. If above transfer tools are employed to translate locations, such as organizing objects from a chaotic scene to an orderly scene in images (i.e., chaos to order), can these tools work well? Therefore, we investigate the issue of image-to-image location transfer and receive a preliminary conclusion that it is hard to achieve location transfer directly on real images. In this paper, we propose a novel framework called LT-GAN to address above issue. Specifically, a multi-stage generation structure is designed, where the location translation is performed based on semantic labels rather than the corresponding raw images for reaching the translation from chaos to order. Experimental results demonstrate the effectiveness of the proposed multi-stage generation strategy. Meanwhile, a Color Histogram Loss is explored to evaluate the similarity of color distribution between a pair of chaotic scene image and the corresponding orderly scene image. In this way, the performance of location trans- fer image synthesis is improved significantly in LT-GAN by using the well-designed combination of a feature extractor and the Color Histogram Loss. Moreover, in order to break through the limitation of previous released datasets, a new dataset named M2C is constructed for this new application scenario of location transfer, including more than 15, 000 paired images and the corresponding semantic labels in total. The dataset is available at https://drive. google.com/open?id=1amr9ga9wvhnIzeZ48OHbLapHGqOb4-Up"
Oral 2,Generative Adversarial Networks with Bi-directional Normalization for Semantic Image Synthesis,Jia Long and Hongtao Lu,"Shanghai Jiao Tong University, Shanghai Jiao Tong University","Semantic image synthesis aims at translating semantic label maps to photo-realistic images. However, most of previous methods easily generate blurred regions and artifacts, and the quality of these images is far from realistic. There are two unresolved problems existing: first, these methods directly feed the semantic label as input to the deep network, through convolution operation to produce the normalization parameters $\lambda$ and $\beta$, we find that the semantic labels are different from real scene images, they are not able to provide detailed structural information, making it difficult to synthesize local details and structures; second, there are no bi-directional information flow between the semantic labels and the real scene images, this leads to inefficiently utilize the semantic information and maintain semantic constrains to preserve the semantic information in the process of semantic image synthesis. We propose Bi-directional Normalization (BDN) in our generative adversarial networks to solve these problems, which allows semantic label information and real scene image feature representation to be effectively utilized by a bi-directional ways for generating high quality images. Extensive experiments on several challenging datasets demonstrate significantly better than the results of existing methods, both visual fidelity and quantitative metrics."
Oral 2,Nested Dense Attention Network for Single Image Super-Resolution,"Cheng Qiu, Yirong Yao and Yuntao Du","Nanjing University, Nanjing University, Nanjing University","Recently, deep convolutional neural networks (CNNs) are widely used in single image super-resolution (SISR) and have recorded impressive performance. However, most of the existing CNNs ar- chitectures can not fully utilize the correlation of feature maps in the middle layers, and abundant features of different levels are lost. Furthermore, convolution operation is limited by processing one local neighborhood at a time, which lacks global information. To ad- dress these issues, we propose the nested dense attention network (NDAN) for generating more refined and structured high-resolution images. Specifically, we propose nested dense structure (NDS) to better integrate features of different levels extracted from different layers. Besides that, in order to capture inter-channel dependen- cies more efficiently, we propose the adaptive channel attention module (ACAM) to adaptively rescale channel-wise features by automatically adjusting the weights of different receptive fields. Furthermore, to better explore the global-level context information, we design hybrid non-local module (HNLM) and hybrid non-local up-sampler (HNLU) to upscale the images by capturing spatial-wise long-distance dependencies and channel-wise long-distance corre- lation. Numerous experiments demonstrate the effectiveness of our model by achieving higher PSNR and SSIM scores and generating images with better structures against the state-of-the-art methods."
Oral 2,Leveraging EfficientNet and Contrastive Learning for Accurate Global-scale Location Estimation,"Giorgos Kordopatis-Zilos, Panagiotis Galopoulos, Symeon Papadopoulos and Ioannis Kompatsiaris","Information Technologies Institute, CERTH, Information Technologies Institute, CERTH, Information Technologies Institute, CERTH, Information Technologies Institute, CERTH","In this paper, we address the problem of global-scale image geolocation, proposing a mixed classification-retrieval scheme. Unlike other methods that strictly tackle the problem as a classification or retrieval task, we combine the two practices in a unified solution leveraging the advantages of each approach with two different modules. The first module leverages the state-of-the-art EfficientNet model to assign images to a specific geographic cell in a robust way. The second module introduces a new residual architecture that is trained with contrastive learning to map input images to an embedding space that minimizes the pairwise geodesic distance of images from the same location. For the final location estimation, the two modules are combined with a search within cell scheme, where the locations of most similar images from the predicted geographic cell are aggregated based on a spatial clustering scheme. Our approach demonstrates very competitive performance on four publicly available datasets, achieving new state-of-the-art performance in fine granularity scales, i.e., 15.0% at 1km range on Im2GPS3k."
Oral 2,GCNBoost: Artwork Classification by Label Propagation through a Knowledge Graph,"Cheikh Brahim El Vaigh, Noa Garcia, Benjamin Renoust, Chenhui Chu, Yuta Nakashima and Hajime Nagahara","Univ Rennes 1, IRISA, National University of Defense Technology, Osaka University, Institute for Datability Science, Median Technologies and Osaka University, Institute for Datability Science, Kyoto University, and Osaka University, Institute for Datability Science, Osaka University, Institute for Datability Science","The rise of digitization of cultural documents offers large-scale contents, opening the road for development of AI systems in order to preserve, search, and deliver cultural heritage. To organize such cultural content also means to classify them, a task that is very familiar to modern computer science. Contextual information is often the key to structure such real world data, and we propose to use it in form of a knowledge graph. Such a knowledge graph, combined with content analysis, enhances the notion of proximity between artworks so it improves the performances in classification tasks. In this paper, we propose a novel use of a knowledge graph, that is constructed on annotated data and pseudo-labeled data. With label propagation, we boost artwork classification by training a model using a graph convolutional network, relying on the relationships between entities of the knowledge graph. Following a transductive learning framework, our experiments show that relying on a knowledge graph modeling the relations between labeled data and unlabeled data allows to achieve state-of-the-art results on multiple classification tasks on a dataset of paintings, and on a dataset of Buddha statues. Additionally, we show state-of-the-art results for the difficult case of dealing with unbalanced data, with the limitation of disregarding classes with extremely low degrees in the knowledge graph."
Oral 3,G-CAM: Graph Convolution Network Based Class Activation Mapping for Multi-label Image Recognition,"Yangtao Wang, Yanzhao Xie and Yu Liu","Guangzhou University, Huazhong University of Science and Technology, Huazhong University of Science and Technology","In most multi-label image recognition tasks, human visual perception keeps consistent for different spatial transforms of the same image. Existing approaches either learn the perceptual consistency with only image-level supervision or preserve the middle-level feature consistency of attention regions but neglect the (global) label dependencies between different objects over the dataset. To address this issue, we integrate graph convolution network (GCN) and propose G-CAM, which learns visual attention consistency via GCN based class attention mapping (CAM) for multi-label image recognition. G-CAM consists of an image feature extraction module to generate the feature maps of the original image and its transformed one and a GCN module to learn weighted classifiers that capture the label dependencies between different objects. Different from previous works which use fully-connected classification layer, G-CAM first fuses weighted classifiers with the feature vector to generate the predicted labels for each input image, then combines weighted classifiers with the feature maps to respectively obtain the transformed attention heatmaps of the original image and the attention heatmaps of its transformed one. We can compute the attention consistency loss according to the distance between these two attention heatmaps. Finally, this loss is combined with the multi-label classification loss to update the whole network in an end-to-end manner. We conduct extensive experiments on three multi-label image datasets including FLICKR25K, MS-COCO and NUS-WIDE. Experimental results demonstrate G-CAM can achieve better performance compared with the state-of-the-art multi-label image recognition methods."
Oral 3,Know Yourself and Know Others: Efficient Common Representation Learning for Few-shot Cross-modal Retrieval,"Shaoying Wang, Hanjiang Lai and Zhenyu Shi","Sun Yat-sen University, Sun Yat-Sen University, Sun Yat-Sen University","Learning the common representations for various modalities of data is the key component in cross-modal retrieval. Most existing deep approaches learn multiple networks to independently project each sample into a common representation. However, each representation is only extracted from the corresponding data, which totally ignores the relationships between other data. Thus it is challenging to learn efficient common representations when lacking sufficient supervised multi-modal data for training, e.g., few-shot cross-modal retrieval. How to efficiently exploit the information contained in other examples is underexplored. In this work, we present the Self-Others Net, a few-shot cross-modal retrieval model that fully exploits information contained both in its own and other samples. First, we propose a self-network to fully exploit the correlations that lurk in the data itself. It integrates the features at different layers and extracts the multi-level information in the self-network. Second, an others-network is further proposed to model the relationships among all samples, which learns the Mahalanobis tensor and mixes the prototypes of all data to capture the non-linear dependencies for common representation learning. Extensive experiments are conducted on three benchmark datasets, which demonstrate clear improvements of the proposed method over the state-of-the-arts."
Oral 3,A Denoising Convolutional Neural Network for Self-Supervised Rank Effectiveness Estimation on Image Retrieval,Lucas Valem and Daniel C. G. Pedronette,"São Paulo State University, São Paulo State University, São Paulo State University","Image and multimedia retrieval has established as a prominent task in an increasingly digital and visual world. Mainly supported by decades of development on hand-crafted features and the success of deep learning techniques, various different feature extraction and retrieval approaches are currently available. However, the frequent requirements for large training sets still remain as a fundamental bottleneck, especially in real-world and large-scale scenarios. In the scarcity or absence of labeled data, choosing what retrieval approach to use became a central challenge. A promising strategy consists in to estimate the effectiveness of ranked lists without requiring any groundtruth data. Most of the existing measures exploit statistical analysis of the ranked lists and measure the reciprocity among lists of images in the top positions. This work innovates by proposing a new and self-supervised method for this task, the Deep Rank Noise Estimator (DRNE). An algorithm is presented for generating synthetic ranked list data, which is modeled as images and provided for training a Convolutional Neural Network that we propose for effectiveness estimation. The proposed model is a variant of the DnCNN (Denoiser CNN), which intends to interpret the incorrectness of a ranked list as noise, which is learned by the network. Our approach was evaluated in 5 public image  datasets and different tasks, including general image retrieval and person re-ID. We also exploited and evaluated the complementary between the proposed approach and related rank-based approaches through fusion strategies. The experimental results showed that the proposed method is capable of achieving up to 0.88 of Pearson correlation with MAP measure in general retrieval scenarios and 0.74 in person re-ID scenarios."
Oral 3,Cross-Modal Image-Recipe Retrieval via Intra- and Inter-Modality Hybrid Fusion,"Jiao Li, Xing Xu, Wei Yu, Jialiang Sun and Fumin She","University of Electronic Science and Technology of China, University of Electronic Science and Technology of China,  University of Electronic Science and Technology of China, University of Electronic Science and Technology of China, University of Electron","Image-recipe retrieval, which aims at retrieving the relevant recipe from a food image and vice versa, is now attracting widespread attention, since sharing food-related images and recipes on the Internet has become a popular trend. Existing methods have formulated this problem as a typical cross-modal retrieval task by learning the image-recipe similarity. Though these methods have made inspiring achievements for image-recipe retrieval, they may still be less effective to jointly incorporate the three crucial points: (1) the association between ingredients and instructions, (2) fine-grained image information, and (3) the latent alignment between recipes and images. To this end, we propose a novel framework named \textit{Intra- and Inter-Modality Hybrid Fusion} (IMHF) to learn accurate image-recipe similarity. Our IMHF model adopts an intra-recipe fusion module to focus on the interaction between ingredients and instructions within a recipe and further enriches the expressions of the two separate embedding. Meanwhile, an image-recipe fusion module is devised to explore the potential relationship between fine-grained image regions and ingredients from the recipe, which jointly forms the final image-recipe similarity from both the local and global aspects.  Extensive experiments on the large-scale benchmark dataset Recipe1M show that our model significantly outperforms the state-of-the-art approaches on various image-recipe retrieval scenarios."
Oral 3,Scene Text Recognition with Cascade Attention Network,"Min Zhang, Meng Ma and Ping Wang","Peking University,Peking University, Ministry of Education","Scene text recognition (STR) has experienced increasing popularity both in academia and in industry. Regarding STR as a sequence prediction task, most state-of-the-art (SOTA) approaches employ the attention-based encoder-decoder architecture to recognize texts. However, these methods still struggle in localizing the precise alignment center associated with the current character, which is also named as attention drift phenomenon. One major reason is that directly converting low-quality or distorted word images to sequential features may introduce confusing information and thus mislead the network. To address the problem, this paper proposes a cascade attention network. The model is composed of three novel attention modules: a vanilla attention module that attends to sequential features from the horizontal direction, a cross-network attention module to take advantage of both one-dimension contextual information and two-dimension visual distributions, and an aspects fusion attention module to fuse spatial and channel-wise information. Accordingly, the network manages to yield distinguished and refined representations correlated to the target sequence. Compared to SOTA methods, experimental results on seven benchmarks demonstrate the superiority of our framework in recognizing scene texts on various conditions. "
Doc.,Rounding Improved DCT Transform Coding for H.266/VVC,Ka-Hou Chan,"Macao Polytechnic Institute, Macao Polytechnic Institute","Many video encoders use DCT transform coding to compress the encoded video. For hardware implementation, DCT will be approximately an integer matrix, which may cause some deviations in this process, and these deviations will accumulate and become obvious in the larger code unit. Our method is to construct all DCT-related discrete orthogonal transforms in the required size (corresponding to the coding unit supported by H.266/VVC). By using a novel discrete orthogonal matrix generation method with determined DCT-II roots, and scaling and rounding a regular DCT that depends on the quantization parameter, instead of integer approximation. We can obtain an accurate integer DCT matrix. Experimental results show that this method can not only improve the video quality and also require fewer bit rates."
Doc.,Fire Detection using Transformer Network,Mohammad Shahid and Kai-Lung Hua,"National Taiwan University of Science and Technology, National Taiwan University of Science and Technology","Technological breakthroughs in computing have empowered vision-based surveillance systems to detect fire using transformers framework. Over the last few decades, convolutional neural networks (CNNs) have been broadly applied for many computer vision-related problems and provided satisfactory results. However, due to the inductive prejudices embedded in convolutional operations, it cannot comprehend long-range dependencies. Vision transformers (ViT) has recently become an alternative to CNN for a vision problem by factoring an image as a patches sequence and leverage intra-attention between pixels. This paper shows that ViT is a viable tool for automated fire detection by aggregating features from the whole spatial context. The proposed method is tested on benchmark fire datasets to reveal the framework's strength and effectiveness"
Doc.,A Beneficial Dual Transformation Approach for Deep Learning Networks Used in Steel Surface Defect Detection,"Fityanul Akhyar, Chih-Yang Lin and Gugan S Kathiresan","Yuan Ze University & Telkom University, Yuan Ze University, Vellore Institute of Technology","Steel surface defect detection represents a challenging task in real-world practical object detection. Based on our observations, there are two critical problems which create this challenge: the tiny size, and vagueness of the defects. To solve these problems, this study a proposes a deep learning-based defect detection system that uses automatic dual transformation in the end-to-end network. First, the original training images in RGB are transformed into the HSV color model to re-arrange the difference in color distribution. Second, the feature maps are upsampled using bilinear interpolation to maintain the smaller resolution. The latest and state-of-the-art object detection model, High-Resolution Network (HRNet) is utilized in this system, with initial transformation performed via data augmentation. Afterward, the output of the backbone stage is applied to the second transformation. According to the experimental results, the proposed approach increases the accuracy of the detection of class 1 Severstal steel surface defects by 3.6% versus the baseline."
SS1,M-DFNet: Multi-phase Discriminative Feature Network for Retrieval of Focal Liver Lesions,"Yingying Xu, Jing Liu, Lanfen Lin, Hongjie Hu, Ruofeng Tong, Jingsong Li and Yen-Wei Chen","Research Center for Healthcare Data Science, Zhejiang Lab, College of Computer Science and Technology, Zhejiang University, Department of Radiology, Sir Run Run Shaw Hospital, Zhejiang University, College of Information Science and Engineering, Ritsumeikan University","Content based medical image retrieval (CBMIR) plays a great role in computer aided diagnosis for assisting radiologists to detect and characterize focal liver lesions (FLLs). Deep learning has gained exciting performance on CBMIR. While the features generated by deep learning models trained using softmax loss are always separable but not discriminative enough, which is insufficient for retrieval task. In this paper, we propose a multi-phase discriminative feature network (M-DFNet) with a DeepExtracter and a feature refine module (FRModule) to learn discriminative and separable features under a joint supervision of center loss and softmax loss. The hybrid loss enables to minimize intra-class variations and enlarge inter-class differences as much as possible. The FRModule is proposed to recalibrate the deep features based on the learned class centers to tackle the complex imaging manifestations of FLLs and further enhance both the feature discrimination and generalization. Multi-phase computed tomography (CT) images contain pivotal information for diagnosis of FLLs. Thus the M-DFNet is designed to cope with multi-phase information and we explore an appropriate and effective method for multi-phase feature integration on limited data. Experimental results clearly demonstrate strong performance superiority by our proposed method."
SS1,Contextualized Keyword Representations for Multi-modal Retinal Image Captioning,"ia-Hong Huang, Ting-Wei Wu and Marcel Worring","University of Amsterdam, Georgia Institute of Technology","Medical image captioning automatically generates a medical description to describe the content of a given medical image. A traditional medical image captioning model creates a medical description only based on a single medical image input. Hence, an abstract medical description or concept is hard to be generated based on the traditional approach. Such a method limits the effectiveness of medical image captioning. Multi-modal medical image captioning is one of the approaches utilized to address this problem. In multi-modal medical image captioning, textual input, e.g., expert-defined keywords, is considered as one of the main drivers of medical description generation. Thus, encoding the textual input and the medical image effectively are both important for the task of multi-modal medical image captioning. In this work, a new end-to-end deep multi-modal medical image captioning model is proposed. Contextualized keyword representations, textual feature reinforcement, and masked self-attention are used to develop the proposed approach. Based on the evaluation of the existing multi-modal medical image captioning dataset, experimental results show that the proposed model is effective with the increase of +53.2% in BLEU-avg and +18.6% in CIDEr, compared with the state-of-the-art method."
SS1,A Tensor Sparse Representation-Based CBMIR System for Computer-Aided Diagnosis of Focal Liver Lesions and its Pilot Trial,"Jian Wang, Xian-Hua Han, Lanfen Lin, Hongjie Hu and Yen-Wei Chen","Shandong Normal University, Yamaguchi University. Zhejiang University, Ritsumeikan University, Peking University","Clinicians refer to diagnosed medical cases in order to make correct diagnosis and take appropriate treatments, due to the complexity of focal liver lesions. It’s a heavy burden, however, for medical doctors to find out similar and meaningful cases from the accumulated extreme large medical datasets. Content based medical image retrieval (CBMIR) that searches for similar images in a large database has been attracting increasing research interest recently. A CBMIR system provides doctors the diagnosed cases to improve the diagnosis accuracy and confidence. This paper proposed a tensor sparse representation method to extract temporal and spatial features of multi-phase CT images, so as to provide doctors medical cases more relevant to the query one. The proposed tensor sparse representation method is applied to the retrieval of focal liver lesions (FLLs). Experiments show that the proposed method achieved better retrieval performance than conventional methods. Pilot trial show that diagnosis accuracy and confidence was improved significantly by the developed CBMIR system based on the proposed method."
Oral 4,Social Relation Analysis from Videos via Multi-entity Reasoning,"Chenghao Yan, Zihe Liu, Fangtao Li, Chenyu Cao, Zheng Wang and Bin Wu","Beijing University of Posts and Telecommunications, Beijing University of Posts and Telecommunications, Beijing University of Posts and Telecommunications, Beijing University of Posts and Telecommunications, Beijing University of Posts and Telecommunications, Beijing University of Posts and Telecommunications","Videos contain rich semantic information. Analyzing social relations in video semantics can help machines interpret the behavior of human beings. However, most of the work related to social relationship recognition is based on still images, while video-based social relationship analysis tasks are less concerned. Here we propose a Multi-entity Relation Reasoning (MRR) framework that can be used for recognizing or predicting social relations in videos. To capture temporal features and contextual cues in videos, and use richer information to represent the person in the video, we track each person's appearance timeline and design a multi-entity representation method to build a social relationship knowledge graph. Then we use graph attention networks to gather information from the entity's neighborhood. Besides, situation information is helpful to identify relationships, we design a situation information extraction module to generate situation embedding from the video clip. Finally, a decoder is adopted to predict relationships between character entities. We evaluate the model on the MovieGraphs dataset and verify the effectiveness of the proposed framework."
Oral 4,Relation-aware Hierarchical Attention Framework for Video Question Answering,"Fangtao Li, Ting Bai, Chenyu Cao, Zihe Liu, Chenghao Yan and Bin Wu","Beijing University of Posts and Telecommunications, Beijing University of Posts and Telecommunications, Beijing University of Posts and Telecommunications, Beijing University of Posts and Telecommunications, Beijing University of Posts and Telecommunications","Video Question Answering (VideoQA) is a challenging video understanding task since it requires a deep understanding of both question and video. Previous studies mainly focus on extracting sophisticated visual and language embeddings, fusing them by the delicate hand-crafted network. However, the relevance of different frames, objects, and modalities to the question are varied along with the time, which is ignored in most of existing methods. Lacking understanding of the the dynamic relationships and interactions among objects brings a great challenge to VideoQA task. To address this problem, we propose a novel Relation-aware Hierarchical Attention (RHA) framework to learn both the static and dynamic relations of the objects in videos. In particular, videos and questions are embedded by pre-trained models firstly to obtain the visual and textutual feares. Then a graph-based relation encoder is utilized to extract the static relationship between visual objects at a single frame. To capture the dynamic changes of multimodal objects in different video frames, we consider three dynamic relations: i.e., the temporal relations, spatial relations, and semantic relations, and fuse the multimodal features by hierarchical attention mechanisms to predict the answer. We conduct extensive experiments on a large scale VideoQA dataset, and the experimental results demonstrate that our RHA outperforms the state-of-the-art methods."
Oral 4,Question-Guided Semantic Dual-Graph Visual Reasoning with Novel Answers,Xinzhe Zhou and Yadong Mu,"Wangxuan Institute of Computer Technology, Peking University, Wangxuan Institute of Computer Technology, Peking University","Visual Question Answering (VQA) has gained increasing attention as being the cross-disciplinary research of computer vision and natural language understanding. However, recent advances mostly treated it as a closed-set classification problem, by limiting the possible outputs to some fixed frequent answers available in a training set. Although effective on benchmark datasets, this paradigm is inherently defective---the VQA model would always fail on a question whose correct answer is out of the answer set, which severely hampers its generalization and flexibility. To try to close the gap, we explore an open-set VQA setting, where models are evaluated using novel samples with unseen answers given dynamic candidate answers from some candidate-generation module.
For experimental purposes, two oracle candidate-sampling strategies are proposed to serve as a proxy for the candidate-generation module and generate dynamic candidate answers for testing samples. The conventional classification-based paradigm is no longer applicable in our setting. To this end, we design a matching based VQA model, in which a novel Single-Source Graph Convolutional Network (SSGCN) module is designed to jointly leverage question guidance and dual semantic answer-graphs to produce more discriminative and relevant answer embeddings. Extensive experiments and ablation studies by re-purposing two benchmark datasets demonstrate the effectiveness of our proposed model."
Oral 4,Heterogeneous Side Information-based Iterative Guidance Model for Recommendation,"Feifei Dai, Xiaoyan Gu, Zhuo Wang, Mingda Qian, Bo Li and Weiping Wang","University of Chinese Academy of Sciences & Institute of Information Engineering, Chinese Academy of Sciences, Institute of Information Engineering, Chinese Academy of Sciences, Sangfor Inc, University of Chinese Academy of Sciences & Institute of Information Engineering, Chinese Academy of Sciences, Institute of Information Engineering, Chinese Academy of Sciences, Institute of Information Engineering, Chinese Academy of Sciences","Heterogeneous side information has been widely used in recommender systems to alleviate the data sparsity problem. However, the heterogeneous side information in existing methods provides insufficient guidance for predicting user preferences as its effect is inevitably weakened during utilization. Furthermore, most existing methods cannot effectively utilize the heterogeneous side information to understand users and items. They often neglect the interrelation among various types of heterogeneous side information of a user or an item. As a result, it is difficult for existing methods to comprehensively understand users and items so that the recommender system recommends inappropriate items to users. To overcome the above drawbacks, we propose an interrelation learning-based recommendation method with iterative heterogeneous side information guidance (ILIG). ILIG includes two modules: 1) Iterative Heterogeneous Side Information Guidance Module. It uses heterogeneous side information to iteratively guide the prediction of user preferences, which effectively enhances the effect of the heterogeneous side information. 2) Interrelation Learning-based Portrait Construction Module. It captures the interrelation among various types of heterogeneous side information to comprehensively learn the representations of users and items. To demonstrate the effectiveness of ILIG, we conduct extensive experiments on Movielens-100K, Movielens-1M, and BookCrossing datasets. The experimental results show that ILIG outperforms the state-of-the-art recommender systems."
Challenge 2,Summary of the 2021 Embedded Deep Learning Object Detection Model Compression Competition for Traffic in Asian Countries,"Yu-Shu Ni, Chia-Chi Tsai, Jiun-In Guo, Jenq-Neng Hwang, Bo-Xun Wu, Po-Chi Hu, Ted T. Kuo, Po-Yu Chen and Hsien-Kai Kuo","National Yang Ming Chiao Tung University, MediaTek Inc, National Yang Ming Chiao Tung University, University of Washington, National Yang Ming Chiao Tung University, Pervasive Artificial Intelligence Research Labs (PAIR Labs), National Yang Ming Chiao Tung University, MediaTek Inc, MediaTek Inc","The 2021 embedded deep learning object detection model compression competition for traffic in Asian countries held in IEEE ICMR2021 Grand Challenges focuses on the object detection technologies in autonomous driving scenarios. The competition aims to detect objects in traffic with low complexity and small model size in the Asia countries (e.g., Taiwan), which contains several harsh driving environments. The target detected objects include vehicles, pedestrians, bicycles and crowded scooters. There are 89,002 annotated images provided for model training and 1,000 images for validation. Additional 5,400 testing images are used in the contest evaluation process, in which 2,700 of them are used in the qualification stage competition, and the rest are used in the final stage competition. There are in total 308 registered teams joining this competition this year, and the top 15 teams with the highest detection accuracy entering the final stage competition, from which 9 teams submitted the final results. The overall best model belongs to team “as798792”, followed by team “Deep Learner” and team “UCBH.” Two special awards of best accuracy award best and bicycle detections go to the same team “as798792,” and the other special award of scooter detection goes to team “abcda.”"
Challenge 2,Bag of Tricks for Building an Accurate and Slim Object Detector for Embedded Applications,"Yongkun Du, Zhineng Chen, Caiyan Jia, Xuanya Li and Yu-Gang Jiang","Beijing Jiaotong University, Fudan University, Beijing Jiaotong University, Baidu Inc., Fudan University","Object detection is an essential computer vision task that possesses extensive application prospects in on-road object detection. In recent years, copious novel methods have been proposed to solve this task. However, the majority of algorithms have high computational cost and thus slow inference speed, making them intractable to be deployed on embedded devices. In this paper, taking YOLOv5s, the smallest model in the YOLOv5 family, as the baseline, we explore a bag of tricks that improve the detection performance with respect to a specified application scenario, under the premise of ensuring that it does not increase the computational cost of YOLOv5s. Specifically, we introduce relevantly external data to deal with the problems of category imbalance. Meanwhile, knowledge distillation is employed to transfer knowledge from a cumbersome model to a compact model, where a novel united distillation scheme is developed to enhance the distillation effectiveness. In addition, a pseudo-label based training strategy is utilized to further learn from the biggest YOLOv5 model while the network network quantization is also applied. We have applied the above tricks to the Embedded Deep Learning Object Detection Model Compression Competition for Traffic in Asian Countries held in conjunction with ICMR 2021. The experiments have shown that all the tricks are useful. They together have built an accurate and slim detection model. It is highly competitive and has been ranked 2nd place in the competition. We believe the tricks are also meaningful for building other application-oriented object detectors."
Challenge 2,Embedded YOLO: Faster and Lighter Object Detection,"Wu Wen-Kai, Chen Chien-Yu and Lee Jiann-Shu","National University of Tainan, National University of Tainan, National University of Tainan","Object detection is a fundamental but very important task in computer vision. Most current algorithms require high computing resources, which hinders their deployment on embedded system. In this research, we propose a neural network model named Embedded YOLO to solve this problem. We propose the DSC_CSP module to replace the middle layers of YOLOv5s to reduce the number of model parameters. On the other hand, in order to avoid the decrease of performance due to the reduction of parameters, we utilize knowledge distillation to maintain performance. To make good use of the information provided by data augmentation, we propose a new method called Dynamic Interpolation Mosaic to improve the original Mosaic. Due to serious imbalance in the number of samples of different data types, we employ a two-stage training scheme to overcome the data imbalance problem. The proposed model achieved the best results in the ICMR2021 Grand Challenge PAIR Competition with 0.59 mAP and model size of 12MB and 108 FPS on the MediaTek's Dimensity 1000 platform. These results confirm that the proposed model is suitable for deployment in embedded systems for object detection task."
Challenge 2,Object Detection for Traffic in Asian Countries based on YOLOv5,Bao-Hong Lai and Hsun-Ping Hsieh,"National Cheng Kung University,National Cheng Kung University","In this paper, we present our proposed 3rd place object detection solution for Embedded Deep Learning Object Detection Model Compression Competition which is held in ICMR 2021. Considering the memory, computing speed, and environmental requirements in the MediaTek Dimensity 1000 Series embedded device, it is worth mentioning that we are the first one to re-implement and propose an efficient object detection algorithm based on You Only Look Once version 5 (YOLOv5) to address this issue in TensorFlow framework. The backbone of our network is mainly constructed by Cross Stage Partial (CSP) modules, which significantly boost the accuracy of our model and keep the model lightweight. Besides, we combine the official training dataset and several external open datasets as our overall training data. We also adopt multiple data augmentation techniques in the training phase, making the model learn a stronger feature extraction ability for various object categories. According to the results of extensive experiments and the final competition scores, our solution can get a not bad performance under the condition of low parameters and complexity."
Best,RGB-D Scene Recognition based on Object-Scene Relation and Semantics-Preserving Attention,Yuhui Guo and Xun Liang,"Renmin University of China, Renmin University of China","Scene recognition is challenging due to intra-class diversity andinter-class similarity. Previous works recognize scenes either withglobal representations or with intermediate representations of objects. By contrast, we investigate more discriminative sequentialrepresentation of object-to-scene relations (SOSRs) for scene recognition. Particularly, we develop an Attention-Preserving MemoryLearning (APML) model, which enforces the Memory Network ofthe semantic domain to guide the Learning Network of the appearance domain in the learning procedure. Accordingly, we allocatesemantics-preserving attention to different objects, which is moreeffective to seek the key encoded SOSR and discard the misleadingencoded SOSR between objects and scene without requiring extralabeled data. Based on the proposed APML networks, we obtain thestate-of-the-art results of RGB-D scene recognition on SUN RGB-Dand NYUD2 datasets."
Best,Reading Scene Text by Fusing Visual Attention with Semantic Representations,"Zhiguang Liu, Liangwei Wang and Jian Qiao","Noah's Ark Lab & Huawei Technologies, Noah's Ark Lab & Huawei Technologies, Huawei Technologies","Recognizing text in an unconstrained environment is a challenging task in computer vision. Many prevalent approaches to it employ a recurrent neural network that is difficult to train or rely heavily on sophisticated model designs for sequence modeling. In contrast to these methods, we propose a unified lexicon-free framework to enhance the accuracy of text recognition using only attention and convolution. We use a relational attention module to leverage visual patterns and word representations. To ensure that the predicted sequence captures the contextual dependencies within a word, we embed linguistic dependencies from a language model into the optimization framework. The proposed mutual attention model is an ensemble of visual cues and linguistic contexts that together improve performance. The results of experiments show that our system achieves state-of-the-art performance on datasets of texts from regular and irregular scenes. It also significantly enhances recognition performance on noisy scanned documents."
Best,Leveraging Two Types of Global Graph for Sequential Fashion Recommendation,"Yujuan Ding, Yunshan Ma, Wai Keung Wong and Tat-Seng Chua","Shenzhen University, National University of Singapore, The Hong Kong Polytechnic University, National University of Singapore","Sequential fashion recommendation is of great significance in online fashion shopping, which accounts for an increasing portion of either fashion retailing or online e-commerce. The key to building an effective sequential fashion recommendation model lies in capturing two types of patterns: the personal fashion preference of users and the transitional relationships between adjacent items. The two types of patterns are usually related to user-item interaction and item-item transition modeling respectively. However, due to the large sets of users and items as well as the sparse historical interactions, it is difficult to train an effective and efficient sequential fashion recommendation model. To tackle these problems, we propose to leverage two types of global graph, i.e., the user-item interaction graph and item-item transition graph, to obtain enhanced user and item representations by incorporating higher-order connections over the graphs. In addition, we adopt the graph kernel of LightGCN for the information propagation in both graphs and propose a new design for item-item transition graph. Extensive experiments on two established sequential fashion recommendation datasets validate the effectiveness and efficiency of our approach."
Best,A Smart Adversarial Attack on Deep Hashing Based Image Retrieval,"Junda Lu, Mingyang Chen, Yifang Sun, Wei Wang, Yi Wang and Xiaochun Yang","University of New South Wales, University of New South Wales, Northeastern University, Dongguan University of Technology. Northeastern University","Deep hashing based retrieval models have been widely used in large-scale image retrieval systems. Recently, there has been a surging interest in studying the adversarial attack problem in deep hashing based retrieval models. However, the effectiveness of existing adversarial attacks is limited by their poor perturbation management, unawareness of ranking weight, and only laser-focusing on the attack image. These shortages lead to high perturbation costs yet low AP reductions. To overcome these shortages, we propose a novel adversarial attack to improve the effectiveness of the adversarial attack. Our attack designs a dimension-wise surrogate Hamming distance to help with wiser perturbation management. Further, in generating adversarial examples, instead of focusing on a single image, we propose to collectively incorporate relevant images combined with an AP-oriented (average precision) weight function. In addition, our attack can deal with both untargeted and targeted adversarial attacks in a flexible manner. Extensive experiments demonstrate that, with the same attack performance, our model significantly outperforms state-of-the-art models in perturbation cost on both untargeted and targeted attack tasks."
Poster,2.5D Pose Guided Human Image Generation,Kang Yuan and Sheng Li,"University of Georgia, Renmin University of China","In this paper, we propose a 2.5D pose guided human image generation method that integrates depth information with 2D poses. In particular, given a specific 2.5D pose and an image of a person, our method is able to generate a new image of that person with a target pose. In order to incorporate depth information into the pose structure, we design a three-layer pose space that allows accurate pose transfer compared with regular 2D pose structure. Specifically, our pose space enables the generative models to address the occlusion problems commonly happened in human image generation and also helps recognize spatial front-back relations of limbs. Our model could be trained end-to-end on images and the corresponding 3D coordinates. The quantitative and qualitative results on the DeepFashion and Human 3.6M datasets demonstrate the effectiveness of our method."
Poster,Weakly Supervised Sketch Based Person Search,"Lan Yan, Wenbo Zheng, Fei-Yue Wang and Chao Gou","Institute of Automation, Chinese Academy of Sciences & University of Chinese Academy of Sciences, Institute of Automation, Chinese Academy of Sciences & University of Chinese Academy of Sciences, Institute of Automation, Chinese Academy of Sciences & University of Chinese Academy of Sciences, Sun Yat-sen University","Person search often requires a query photo of the target person. However, in many practical scenarios, there is no guarantee that such a photo is always available. In this paper, we define the problem of sketch based person search, which uses a sketch instead of a photo as the probe for retrieving. We tackle this problem in a weak supervision setting and propose a clustering and feature attention based weakly supervised learning framework, which contains tow stages of pedestrian detection and sketch based person re-identification. Specially, we introduce multiple detectors, followed by fuzzy c-means clustering to achieve weakly supervised pedestrian detection. Moreover, we design an attention module to learn discriminative features in subsequent re-identification network. Extensive experiments show the superiority of our method."
Poster,Personal Knowledge Base Construction from Multimodal Data,"An-Zi Yen, Chia-Chun Chang, Hen-Hsen Huang and Hsin-Hsi Chen","National Taiwan University, National Taiwan University, Academia Sinica & MOST Joint Research Center for AI Technology and All Vista Healthcare, National Taiwan University & MOST Joint Research Center for AI Technology and All Vista Healthcare","With the passage of time, people often have misty memories of their past experiences. Information recall support for people by collecting personal lifelogs is emerging. Recently, people tend to record their daily life via filming Video Weblog (VLog), which contains visual and audio data. These large scale multimodal data can be used to support information recall service that enables users to query their past experiences. The challenging issue is the semantic gap between the visual concept and the textual query. In this paper, we aim to extract personal life events from vlogs shared on YouTube and construct a personal knowledge base for individuals. A multitask learning model is proposed to extract the components of personal life events, such as subjects, predicates and objects. The evaluation is performed on a video collection from three YouTubers who are English native speakers. Experimental results show our model achieves promising performance."
Poster,NMS-Loss: Learning with Non-Maximum Suppression for Crowded Pedestrian Detection,"Zekun Luo, Zheng Fang, Sixiao Zheng, Yabiao Wang and Yanwei Fu","Youtu Lab, Tencent, Beihang University, Academy for Engineering Technology, Fudan University, Youtu Lab, Fudan University","Non-Maximum Suppression (NMS) is essential for object detection and affects the evaluation results by incorporating False Positives (FP) and False Negatives (FN), especially in crowd occlusion scenes. In this paper, we raise the problem of weak connection between the training targets and the evaluation metrics caused by NMS and propose a novel NMS-Loss making the NMS procedure can be trained end-to-end without any additional network parameters. Our NMS-Loss punishes two cases when FP is not suppressed and FN is wrongly eliminated by NMS. Specifically, we propose a pull loss to pull predictions with the same target close to each other, and a push loss to push predictions with different targets away from each other. We apply NMS-Loss in our baseline detector for pedestrian detection, namely NMS-Ped. Experimental results show that with the help of NMS-Loss, our NMS-Ped gains 10% performance improvement on Caltech dataset and CityPersons dataset compared with the baseline, achieving impressive results with Miss Rate of 5.92% on Caltech dataset and 10.08% on CityPersons dataset, which are both better than state-of-the-art competitors."
Poster,Image Retrieval by Hierarchy-aware Deep Hashing Based on Multi-task Learning,"Bowen Wang, Liangzhi Li, Yuta Nakashima, Takehiro Yamamoto, Hiroaki Ohshima, Yoshiyuki Shoji, Kenro Aihara and Noriko Kando","Osaka University, Osaka University, University of Hyogo, National Institute of Informatics, Aoyama Gakuin University, National Institute of Informatics, Aoyama Gakuin University, National Institute of Informatics, The Graduate University for Advanced Studies","Deep hashing has been widely used to approximate nearest-neighbor search for image retrieval tasks. Most of them are trained with image-label pairs without any inter-label relationship, which may not make full use of the real-world data. This paper presents deep hashing, named HA2HS, that leverages multiple types of labels with hierarchical structures that an ethnological museum assigns to their artifacts. We experimentally prove that HA2HS can learn to generate hashes that give a better retrieval performance."
Poster,Spatio-Temporal Activity Detection and Recognition in Untrimmed Surveillance Videos,"	Konstantinos Gkountakos, Despoina Touska, Konstantinos Ioannidis, Theodora Tsikrika, Stefanos Vrochidis and Ioannis Kompatsiaris","Information Technologies Institute - Centre for Research and Technology Hellas, Information Technologies Institute - Centre for Research and Technology Hellas, Information Technologies Institute - Centre for Research and Technology Hellas, Information Technologies Institute - Centre for Research and Technology Hellas, Information Technologies Institute - Centre for Research and Technology Hellas, Information Technologies Institute - Centre for Research and Technology Hellas","This work presents a spatio-temporal activity detection and recognition framework for untrimmed surveillance videos consisting of a three-step pipeline: object detection, tracking, and activity recognition. The framework relies on the YOLO v4 architecture for object detection, Euclidean distance for tracking, while the activity recognizer uses a 3D Convolutional Deep learning architecture employing spatio-temporal boundaries and addressing it as multi-label classification. The evaluation experiments on the VIRAT dataset achieve accurate detections of the temporal boundaries and recognitions of activities in untrimmed videos, with better performance for the multi-label compared to the multi-class activity recognition."
Poster,Cross-Modal Self-Attention with Multi-Task Pre-Training for Medical Visual Question Answering,"Haifan Gong, Guanqi Chen, Sishuo Liu, Yizhou Yu and Guanbin Li","Sun Yat-sen University, Sun Yat-sen University, The University of Hong Kong, The University of Hong Kong","Due to the severe lack of labeled data, existing methods of medical visual question answering  usually rely on transfer learning to obtain effective image feature representation and use cross-modal fusion of visual and linguistic features to achieve question-related answer prediction. These two phases are performed independently and without considering the compatibility and applicability of the pre-trained features for cross-modal fusion. In this paper, we introduce a cross-modal self-attention (CMSA) module to selectively capture the long-range contextual relevance for more effective fusion of visual and linguistic features. More importantly, we propose to reformulate image feature pre-training as a multi-task learning paradigm and witness its extraordinary superiority, forcing it to take into account the applicability of features for both the specific image comprehension task and our customized CMSA module during  training. Experimental results demonstrate that the proposed method  outperforms existing state-of-the-art methods. Code will be made available."
Poster,Evaluating Contrastive Models for Instance-based Image Retrieval,"Tarun Krishna, Kevin McGuinness and Noel O'Connor","Dublin City University, Dublin City University, Dublin City University","In this work, we evaluate contrastive models for the task of image retrieval. We hypothesise that models that are learned to encode semantic similarity among  instances via discriminative learning should perform well on the task of image retrieval, where relevancy is defined in terms of instances of the same object. Through our extensive evaluation, we find that representations from models trained using contrastive methods perform on-par with (and outperforms) a supervised baseline trained on the ImageNet labels in retrieval tasks under various configurations. This is remarkable given that the contrastive models require no explicit supervision. Thus, we conclude that these models can be used to bootstrap base models to build more robust image retrieval engines."
Poster,Unsupervised Video Summarization via Multi-source Features,"Hussain Kanafani, Junaid Ahmed Ghauri, Sherzod Hakimov and Ralph Ewerth","Leibniz University Hannover, TIB – Leibniz Information Centre for Science and Technology","Video summarization aims at generating a compact yet representative visual summary that conveys the essence of the original video. The advantage of unsupervised approaches is that they do not require human annotations to learn the summarization capability and generalize to a wider range of domains. Previous work relies on the same kind of deep features, typically based on a model pre-trained on ImageNet data. Therefore, we propose the incorporation of multiple feature sources with chunk and stride fusion to provide more information about the visual content. For a comprehensive evaluation on the two benchmarks TVSum and SumMe, we compare our method with four state-of-the-art approaches. Two of these approaches were implemented by ourselves to reproduce the reported results. Our evaluation shows that we obtain state-of-the-art results on both datasets, while also highlighting the shortcomings of previous work with regard to the evaluation methodology. Finally, we perform an error analysis on videos of the two benchmark datasets to summarize and spot the factors that lead to misclassifications."
Poster,Text-Enhanced Attribute-Based Attention for Generalized Zero-Shot Fine-Grained Image Classification,Yan-He Chen and Mei-Chen Yeh,"National Taiwan Normal University, National Taiwan Normal University","We address the problem of generalized zero-shot fine-grained image classification, in which classes are visually similar and training images for some classes are not available. We leverage auxiliary information in the form of textual descriptions to facilitate the task. Specifically, we propose a text-enhanced attribute-based attention mechanism to compute features from the most relevant image regions guided from the most relevant attributes. Experiments on two popular datasets of CUB and AWA2 show the effectiveness of the proposed method."
Poster,Learning to Select: A Fully Attentive. Approach for Novel Object Captioning,"Marco Cagrandi, Marcella Cornia, Matteo Stefanini, Lorenzo Baraldi, Rita Cucchiara","University of Modena and Reggio Emilia, University of Modena and Reggio Emilia, University of Modena and Reggio Emilia, University of Modena and Reggio Emilia, University of Modena and Reggio Emilia","Image captioning models have lately shown impressive results when applied to standard datasets and have proven their value for retrieval and multimedia applications. Switching to real-life scenarios, however, constitutes a challenge due to the larger variety of visual concepts that need to be described, and which are not covered in existing training sets. For this reason, novel object captioning (NOC) has recently emerged as a paradigm to test captioning models on objects which are unseen during the training phase. In this paper, we present a novel approach for NOC that learns to select the most relevant objects of an image, regardless of their adherence to the training set, and to constrain the generative process of a language model accordingly. Our architecture is fully-attentive and end-to-end trainable, also when incorporating constraints. We perform experiments on the held-out COCO dataset, where we demonstrate the effectiveness of the proposed method and improvements over the state of the art, both in terms of adaptability to novel objects and caption quality."
Poster,Semi-supervised Many-to-many Music Timbre Transfer,"Yu-Chen Chang, Wen-Cheng Chen and Min-Chun Hu","National Cheng Kong University, National Cheng Kong University, National Tsing Hua University","This work presents a music timbre transfer model that aims to transfer the style of a music clip while preserving the semantic content. Compared to the existing music timbre transfer models, our model can achieve many-to-many timbre transfer between different instruments. The proposed method is based an autoencoder framework, which comprises two pretrained encoders trained in a supervised manner and one decoder trained in an unsupervised manner. To learn more representative features for the encoders, we produced a parallel dataset, called MI-Para, which is synthesized from MIDI files and digital audio workstations (DAW). Both the objective and the subjective evaluation results showed the effectiveness of the proposed framework. To scale up the application scenario, we also demonstrate that our model can achieve style transfer by training in a semi-supervised manner with a smaller parallel dataset."
Poster,Collaborative Representation for Deep Meta Metric Learning,"Min Zhu, Weifeng Liu, Kai Zhang, Ye Li and Baodi Liu","China University of Petroleum (East China), China University of Petroleum (East China), China University of Petroleum (East China), Qilu University of Technology (Shandong Academy of Sciences), China University of Petroleum (East China)","In this paper, we propose a deep meta metric learning method
based on collaborative representation for person and vehicle re-identification. Most existing deep metric learning methods employ all training data to construct a single metric, and it usually over-fit on the ""salient"" feature. To address this problem, we construct multiple episodes from the original training set to learn a general metric, where each episode is randomly divided into a support set and a query set. Then, we introduce a collaborative representation method, which fits the query sample with the support samples for each class. We predict the query sample's label via the optimal fitness among the query sample and the support samples in each specific class. Besides, we adopt a hard mining strategy to learn a more discriminative metric according to increasing the training tasks' difficulty. Experimental results demonstrate that our method achieves state-of-the-art performance on several re-ID benchmark datasets."
Poster,AWFA-LPD: Adaptive Weight Feature Aggregation for Multi-frame License Plate Detection,"Xiaocheng Lu, Yuan Yuan and Qi Wang","School of Computer Science and School of Artificial Intelligence, Optics and Electronics (iOPEN), School of Computer Science and School of Artificial Intelligence, Optics and Electronics (iOPEN), School of Computer Science and School of Artificial Intelligence, Optics and Electronics (iOPEN)","For license plate detection (LPD), most of the existing work is based on images as input. If these algorithms can be applied to multiple frames or videos, they can be adapted to more complex unconstrained scenes. In this paper, we propose a LPD framework for detecting license plates in multiple frames  or videos, called AWFA-LPD, which effectively integrates the features of nearby frames. Compared with image based detection models, our network integrates optical flow extraction module, which can propagate the features of local frames and fuse with the reference frame. Moreover, we concatenate a non-link suppression module after the detection results to post-process the bounding boxes. Extensive experiments demonstrate the effectiveness and efficiency of our framework."
Poster,Body Shape Calculator: Understanding the Type of Body Shapes from Anthropometric Measurements,Shintami Chusnul Hidayati and Yeni Anistyasari,"Institut Teknologi Sepuluh Nopember, Universitas Negeri Surabaya","Human body shape, which describes the contours of the body figure as well as the distribution of muscles and fat, contains a rich source of information, from health issues to aesthetic presentation of fashion styles. However, most of the existing methods for estimating body types are based on subjective measures, which are susceptible to multiple biases. Determining the type of body shapes is still a challenging analytical task, for which open questions remain regarding good feature representation and classification methods, given noisy and imbalanced real-world data. In this work, we propose a novel body type recognition framework based on anthropometric measurements, which integrates label filtering and pseudo-feature synthesis modules. Label filtering is proposed to identify and filter out potentially noisy labels during classifier training, while pseudo-feature is generated to improve feature representation. Experimental results on the collected dataset from online feeds demonstrate the effectiveness of the approach compared to the state-of-the-art baselines."
Poster,Reproducibility Companion Paper: Knowledge Enhanced Neural Fashion Trend Forecasting,"Yunshan Ma, Yujuan Ding, Xun Yang, Lizi Liao, Wai Keung Wong and Tat-Seng Chua","National University of Singapore, The Hong Kong Polytechnic University","This companion paper supports the replication of the fashion trend forecasting experiments with the KERN (Knowledge Enhanced Recurrent Network) method that we presented in the ICMR 2020. We provide an artifact that allows the replication of the experiments using a Python implementation. The artifact is easy to deploy with simple installation, training, and evaluation. We reproduce the experimental results provided by the original paper and obtained similar performance as previously reported. The replication results of the experiments support the main claims in the original paper."
Demo,Automatic Baseball Pitch Overlay,Ting-Hsuan Chou and Wei-Ta Chu,"National Cheng Kung University, National Cheng Kung University","To provide rich viewing experience and assist pitcher training, we propose an automatic baseball pitch overlay system in this paper. Given multiple pitching video sequences, this system detects and tracks the ball to construct ball trajectories. Because of occlusion, motion blur, and background noise, the ball usually cannot be detected successfully. We propose a series of processes like initial compensation and polynomial fitting to construct complete trajectories. To make the overlay results more appealing, different sequences are weighted differently, and different trajectories are intentionally drawn in different colors. We believe this would be the first fully-automatic pitch overlay system that only takes pitching videos as inputs. Source code is at https://github.com/chonyy/ML-auto-baseball-pitching-overlay. "
Demo,Video Action Retrieval Using Action Recognition Model,Yuko Iinuma and Shin'Ichi Satoh,"The University of Tokyo, National Institute of Informatics","In addition to video sharing services such as YouTube, the spread of video-based social networking services such as TikTok and Instagram have led to the accumulation of vast amounts of video data. In this situation, it has become important to develop a technology to efficiently retrieve necessary information from the accumulated video data. In this paper, we propose a method to retrieve similar videos by focusing on people and their actions in the videos. We apply this method to various datasets and TV videos to demonstrate its usefulness. Demonstration video: http://www.satoh-lab.nii.ac.jp/member/iinuma/ICMR2021_demo.mp4"
Demo,IR Questioner: QA-based Interactive Retrieval System,"Rintaro Yanagi, Ren Togo, Takahiro Ogawa and Miki Haseyama","Hokkaido University, Hokkaido University, Hokkaido University, Hokkaido University","Image retrieval from a given text query (text-to-image retrieval) is one of the most essential systems, and it is effectively utilized for databases on the Web. To make them more versatile and familiar, a retrieval system that is adaptive even for personal DBs such as images in smartphones and lifelogging devices should be considered. In this paper, we present a novel text-to-image retrieval system that is specialized for personal DBs. With the cross-modal scheme and the question-answering scheme, the developed system enables users to obtain the desired image effectively even from personal DBs."
Demo,MeTILDA: Platform for Melodic Transcription in Language Documentation and Application,"Mitchell Lee, Praveena Avula and Min Chen","University of Washington Bothell, University of Washington Bothell, University of Washington Bothell","Blackfoot language is an endangered language that needs to be documented, analyzed, and preserved. As a pitch accent language, Blackfoot is challenging to learn and teach. Words that are spelled the same take on different meanings when changing in pitch. Linguistics researchers are working to create visual aids, called Pitch Art, to teach the nuance in pitch changes. However, the existing techniques used to create Pitch Art require time-consuming work across multiple applications. Additionally, new forms of audio analysis are required in order to create visuals that accurately indicate changes in pitch. To support the preservation of the language, this project proposes a system that provides new forms of audio analysis and automates the process of creating Pitch Art. This application provides value to a variety of stakeholders. Linguistics researchers are provided with tools to analyze and compare Blackfoot speeches. Teachers are given collections of words and recordings from native speakers to teach students. Students are given the ability to compare their own pronunciation of Blackfoot words to that of native speakers. We also discuss a new form of audio analysis, called a perceptual scale, that is used to provide more effective visuals of perceived changes in pitch movement. By collaborating with domain experts in this field, we have validated the effectiveness of using this system to create Pitch Art using the perceptual scale. The analysis can help reveal patterns in Blackfoot words and speech, which in turn helps analyze and preserve the language."
Oral 5,Multi-Initialization Graph Meta-Learning for Node Classification,"Feng Zhao, Donglin Wang and Xintao Xiang","Zhejiang University & Westlake University, Westlake University & Westlake Institule for Advanced Study, Australian National University","Meta-learning aims to acquire common knowledge from a large amount of similar tasks and then adapts to unseen tasks within few gradient updates. Existing graph meta-learning algorithms show appealing performance in a variety of domains such as node classification and link prediction. These methods find a single common initialization for entire tasks and ignore the diversity of task distributions, which might be insufficient for multi-modal tasks. Recent approaches adopt modulation network to generate task-specific parameters for further achieving multiple initializations, which shows excellent performance for multi-modal image classification. However, different from image classification, how to design an effective  modulation network to handle graph-structure dataset is still challenging. In this paper, we propose a Multi-Initialization Graph Meta-Learning (MI-GML) network for graph node classification, mainly consisting of local and global modulation neworks and meta learner. In terms of modulation network, we exploit local and global graph structure information to extract task-specific modulation parameters. On this basis, the meta learner is further modulated by the corresponding modulation parameter to produce task-specific representation for node classification. Experimental results on three graph-structure datasets demonstrate the effectiveness of MI-GML in few-shot node classification tasks."
Oral 5,MLFont: Few-Shot Chinese Font Generation via Deep Meta-Learning,"Xu Chen, Lei Wu, Minggang He, Lei Meng and Xiangxu Meng","Shandong University, Shandong University, Shandong Survey and Design Institute of Water Conservancy, Shandong University & Key Laboratory of Shandong Province for Software Engineering, Shandong University","The automatic generation of Chinese fonts is challenging due to the large quantity and complex structure of Chinese characters. When there are insufficient reference samples for the target font, existing deep learning-based methods cannot avoid overfitting caused by too few samples, resulting in blurred glyphs and incomplete strokes. To address these problems, this paper proposes a novel deep meta-learning-based font generation method (MLFont) for few-shot Chinese font generation, which leverages existing fonts to improve the generalization capability of the model for new fonts. Existing deep meta-learning methods mainly focus on few-shot image classification. To apply meta-learning to font generation, we present a meta-training strategy based on Model-Agnostic Meta-Learning (MAML) and a task organization method for font generation. The meta-training makes the font generator easy to fine-tune for new font generation tasks. Through random font generation tasks and extraction of glyph content and style separately, the font generator learns the prior knowledge of character structure in the meta-training stage, and then quickly adapts to the generation of new fonts with a few samples by fine-tuning of adversarial training. Extensive experiments demonstrate that our method outperforms the state-of-the-art methods with more complete strokes and less noise in the generated character images."
Oral 5,Combining Adversarial and Reinforcement Learning for Video Thumbnail Selection,"Evlampios Apostolidis, Eleni Adamantidou, Vasileios Mezaris and Ioannis Patras","Information Technologies Institute - Centre for Research and Technology, Hellas","This paper presents a new method for unsupervised video thumbnail selection. The developed network architecture selects video thumbnails based on two criteria: the representativeness and the aesthetic quality of their visual content. Training is based on a combination of adversarial and reinforcement learning. The former is used to train a discriminator, whose goal is to distinguish the original from a reconstructed version of the video based on a small set of candidate thumbnails. The received feedback from the discriminator is a measure of the representativeness of the selected thumbnails. This measure is used in combination with estimates about the aesthetic quality of the thumbnails (made using a pretrained model of a SoA Fully Convolutional Network) to form a reward and train the thumbnail selector via reinforcement learning. Experiments on two datasets (OVP and Youtube) demonstrate the competitiveness of the proposed method against other SoA approaches. An ablation study with respect to the adopted criteria for thumbnail selection shows the importance of considering the aesthetic quality when selecting video thumbnails, and the contribution of this information when used in combination with measures about the representativeness of the visual content."
Oral 5,Aligning Visual Prototypes with BERT Embeddings for Few-Shot Learning,"Kun Yan, Zied Bouraoui, Ping Wang, Shoaib Jameel and Steven Schockaert","Peking University, CRIL CNRS & Univ Artois, University of Essex, Cardiff University","Few-shot learning (FSL) is the task of learning to recognize previously unseen categories of images from a small number of training examples. This is a challenging task, as the available examples may not be enough to unambiguously determine which visual features are most characteristic of the considered categories. To alleviate this issue, we propose a method that additionally takes into account the names of the image classes. While the use of class names has already been explored in previous work, our approach differs in two key aspects. First, while previous work has aimed to directly predict visual prototypes from word embeddings, we found that better results can be obtained by treating visual and text-based prototypes separately. Second, we propose a simple strategy for learning class name embeddings using the BERT language model, which we found to substantially outperform the GloVe vectors that were used in previous work. We furthermore propose a strategy for dealing with the high dimensionality of these vectors, inspired by models for aligning cross-lingual word embeddings. We provide experiments on miniImageNet, CUB and tieredImageNet, showing that our approach consistently improves the state-of-the-art in metric-based FSL. "
Oral 5,HINFShot: A Challenge Dataset for Few-Shot Node Classification in Heterogeneous Information Network,"Zifeng Zhuang, Xintao Xiang, Siteng Huang and Donglin Wang","Westlake University, The Australian National University","Few-shot learning aims to generalize to novel classes. It has achieved great success in image and text classification tasks. Inspired by such success, few-shot node classification in homogeneous graph has attracted much attention but few works have begun to study this problem in Heterogeneous Information Network (HIN) so far. We consider few-shot learning in HIN and study a pioneering problem HIN Few-Shot Node Classification (HIN-FSNC) that aims to generalize the node types with sufficient labeled samples to unseen node types with only few-labeled samples. However, existing HIN datasets contain just one labeled node type, which means they cannot meet the setting of unseen node types. To facilitate the investigation of HIN-FSNC, we propose a large-scale academic HIN dataset called HINFShot. It contains 1,235,031 nodes with four node types (author, paper, venue, institution) and all the nodes regardless of node type are divided into 80 classes. Finally, we conduct extensive experiments on HINFShot and the result indicates a significant challenge of identifying novel classes of unseen node types in HIN-FSNC."
Oral 6,Dense Scale Network for Crowd Counting,"Feng Dai, Hao Liu, Yike Ma, Xi Zhang and Qiang Zhao","Beijing University of Posts and Telecommunications & Institute of Computing Technology, Chinese Academy of Sciences, Institute of Computing Technology, Chinese Academy of Sciences & University of Chinese Academy of Sciences, Institute of Computing Technology, Beijing University of Posts and Telecommunications, Institute of Computing Technology, Chinese Academy of Sciences","Crowd counting has been widely studied by computer vision community in recent years. Due to the large scale variation, it remains to be a challenging task. Previous methods adopt either multi-column CNN or single-column CNN with multiple branches to deal with this problem. However, restricted by the number of columns or branches, these methods can only capture a few different scales and have limited capability. In this paper, we propose a simple but effective network called DSNet for crowd counting, which can be easily trained in an end-to-end fashion. The key component of our network is the dense dilated convolution block, in which each dilation layer is densely connected with the others to preserve information from continuously varied scales. The dilation rates in dilation layers are carefully selected to prevent the block from gridding artifacts. To further enlarge the range of scales covered by the network, we cascade three blocks and link them with dense residual connections. We also introduce a novel multi-scale density level consistency loss for performance improvement. To evaluate our method, we compare it with state-of-the-art algorithms on five crowd counting datasets (ShanghaiTech, UCF-QNRF, UCF_CC_50, UCSD and WorldExpo'10). Experimental results demonstrate that DSNet can achieve the best overall performance and make significant improvements."
Oral 6,Distractor-Aware Tracker with A Domain-Special Optimized Benchmark for Soccer Player Tracking,"Zikai Song, Zhiwen Wan, Wei Yuan, Ying Tang, Junqing Yu and Yi-Ping Phoebe Chen","Huazhong University of Science and Technology, Huazhong University of Science and Technology, Huazhong University of Science and Technology, Huazhong University of Science and Technology, La Trobe University","Player tracking in broadcast soccer videos has received widespread attention in the field of sports video analysis, however, we note that there is not a suitable tracking algorithm specifically for soccer video, and the existing benchmarks used for soccer player tracking cover few scenarios with low difficulties. From the observation of the soccer scene that interference and occlusion are knotty problems because the distractors are extremely similar to the targets, a distractor-aware player tracking algorithm and a high-quality benchmark for soccer play tracking (BSPT) have been presented. The distractor-aware player tracking algorithm is able to perceive semantic information about distracting players in the background by similarity judgment, the semantic distractor-aware information is encoded into a context vector and is constantly updated as the objects move through a video sequence. Distractor-aware information is then appended to the tracking results of the baseline tracker to improve the intra-class discriminative power. BSPT contains a total of 120 sequences with rich annotations. Each sequence covers 8 specialized frame-level challenges from soccer scenarios and the player occlusion situations are finely divided into 4 categories for a more comprehensive comparison. In the experimental section, the performance of our algorithm and the other 14 compared trackers is evaluated on BSPT with detailed analysis. Experimental results reveal the effectiveness of the proposed distractor-aware model especially under the challenges of occlusion."
Oral 6,Efficient Indexing of 3D Human Motions,"Petra Budikova, Jan Sedmidubsky and Pavel Zezula","Masaryk University, Masaryk University, Masaryk University","Digitization of human motion using 2D or 3D skeleton representations offers exciting possibilities for many applications but, at the same time, requires scalable content-based retrieval techniques to make such data reusable. Although a lot of research effort focuses on extracting content-preserving motion features, there is a lack of techniques that support efficient similarity search on a large scale. In this paper, we introduce a new indexing scheme for organizing large collections of spatio-temporal skeleton sequences. Specifically, we apply the motion-word concept to transform skeleton sequences into structured text-like motion documents, and index such documents using an extended inverted-file approach. Over this index, we design a new similarity search algorithm that exploits the properties of the motion-word representation and provides efficient retrieval with a variable level of approximation, possibly reaching constant search costs disregarding the collection size. Experimental results confirm the usefulness of the proposed approach."
Oral 6,Few-Shot Action Localization without Knowing Boundaries,"Tingting Xie, Christos Tzelepis, Fan Fu and Ioannis Patras","Queen Mary University of London, Queen Mary University of London, City, University of London, Queen Mary University of London","Learning to localize actions in long, cluttered, and untrimmed videos is a hard task, that in the literature has typically been addressed assuming the availability of large amounts of annotated training samples for each class -- either in a fully-supervised setting, where action boundaries are known, or in a weakly-supervised setting, where only class labels are known for each video. In this paper, we go a step further and show that it is possible to learn to localize actions in untrimmed videos when a) only one/few trimmed examples of the target action are available at test time, and b) when a large collection of videos with only class label annotation (some trimmed and some weakly annotated untrimmed ones) are available for training; with no overlap between the classes used during training and testing. To do so, we propose a network that learns to estimate Temporal Similarity Matrices (TSMs) that model a fine-grained similarity pattern between pairs of videos (trimmed or untrimmed), and uses them to generate Temporal Class Activation Maps (TCAMs) for seen or unseen classes. The TCAMs serve as temporal attention mechanisms to extract video-level representations of untrimmed videos, and to temporally localize actions at test time. To the best of our knowledge, we are the first to propose a weakly-supervised, one/few-shot action localization network that can be trained in an end-to-end fashion. Experimental results on THUMOS14 and ActivityNet1.2 datasets, show that our method achieves performance comparable or better to state-of-the-art fully-supervised, few-shot learning methods."
Oral 6,Facial Structure Guided GAN for Identity-preserved Face Image De-occlusion,"Yiu-Ming Cheung, Mengke Li and Rong Zou","Hong Kong Baptist University, Hong Kong Baptist University","In some practical scenarios, such as video surveillance and personal identification, we often need to address the recognition problem of occluded faces, where content replacement by serious occlusion with non-face objects always produces partial appearance and ambiguous representation. Therefore, the performance of face recognition algorithms will deteriorate to a certain degree. In this paper, we address this problem by removing occlusions on face images and present a new two-stage Facial Structure Guided Generative Adversarial Network (FSG-GAN). In Stage I of the FSG-GAN, the variational auto-encoder is used to predict the facial structure. In Stage II, the predicted facial structure and the occluded image are concatenated together and fed into a generative adversarial network (GAN) based model to synthesize the de-occlusion face image. In this way, the facial structure knowledge can be transferred to the synthesis network. Especially, in order to enable the occluded face image to be perceived as a whole, the generator in the GAN based synthesis network utilizes the hybrid dilated convolution modules to extend the receptive field. Furthermore, aiming at further eliminating the appearance ambiguity as well as unnatural texture, a multi-receptive fields discriminator is proposed to utilize the features from different levels. Experiments on the benchmark datasets show the efficacy of the proposed FSG-GAN."
SS2,Human Pose Estimation based on Attention Multi-resolution Network,"Congcong Zhang, Ning He, Qixiang Sun, Xiaojie Yin and Ke Lu","Beijing Union University, Beijing Union University, Beijing Union University, Beijing Union University, University of Chinese Academy of Sciences","Recently, multi-resolution neural networks, which combine features of different resolutions, have achieved good results in human pose estimation tasks. In this paper, we propose an attention-mechanism-based multi-resolution network, which adds an attention mechanism to the High-Resolution Network (HRNet) to enhance the feature representation of the network. It improves the ability of networks with different resolutions to extract key features from images, and causes the output to contain more effective multi-resolution representation information, so that the corresponding point positions of human joints can be estimated more accurately. Experiments on the MPII and COCO datasets, and verification on the MPII datasets, obtained an average accuracy of 90.3% under the PCKh@0.5 evaluation standard, and good results were also achieved on the COCO dataset (with an AP of 76.5). The experimental results show that our network model is effective in improving the accuracy of key point estimation in the human pose estimation task."
SS2,MSAV: An Unified Framework for Multi-view Subspace Analysis with View Consistence,"Huibing Wang, Guangqi Jiang, Jinjia Peng and Xianping Fu","Dalian Maritime University, Dalian Maritime University, Hebei University, Dalian Maritime University & Pengcheng Laboratory","With the development of multimedia period, information is always caputred with multiple views, which causes a research upsurge on multi-view learning. It is obvious that multi-view data contains more information than those single view ones. Therefore, it is crucial to develop the multi-view algorithms to adapt the demand of many applications. Even though some excellent multi-view algorithms were proposed, most of them can only deal with the specific problems.   To tacle this problem, this paper proposes an unified framework named Multi-view Subspace Analysis with View Consistence (MSAV), which provides an unified means to extend those single-view dimension reduciton algorithms into multi-view versions. MSAV first extends multi-view data into kernel space to avoid the problem caused by different dimensions of the data from multiple views. Then, we introduced a self-weighted learning strategy to automatically assign weights for all views according to their importance. Finally, in order to promote the consistence of all views, Hilbert-Schmidt Independence Criterion is adopted by MSAV. Furthermore, We conducted experiments on several benchmark datasets to verify the performance of MSAV."
SS2,M2GUDA: Multi-Metrics Graph-Based Unsupervised Domain Adaptation for Cross-Modal Hashing,"Chengyuan Zhang, Zhi Zhong, Lei Zhu, Shichao Zhang, Da Cao and Jianfeng Zhang","Hunan University, Hunan University, Hunan Agricultural University, Central South University, Hunan University, National University of Defense Technology","Cross-modal hashing is a critical but very challenging task that is to retrieve similar samples of one modality via queries of other modalities. To improve the unsupervised cross-modal hashing, domain adaptation techniques can be used to support unsupervised hashing learning by transferring semantic knowledge from labeled source domain to unlabeled target domain. However, there are two problems that cannot be ignored: (1) most of domain adaptation based researches mainly focused on unimodal hashing or cross-modal real value-based retrieval but the study for cross-modal hashing is limited; (2) most existing studies only consider one or two consistency constraints during the domain adaptation learning. To this end, this paper propose a novel end-to-end framework to realize unsupervised domain adaptation for cross-modal hashing. This method, dubbed M2GUDA, including four different consistency constraints: structure consistency, domain consistency, semantic consistency and modality consistency for domain adaptation learning. Besides, to enhance the structure consistency learning, we develop a multi-metrics graph modeling method to capture structure information comprehensively. Extensive experiments are performed on three common used benchmarks to evaluate the effectivity of our method. The results show that our method outperforms several state-of-the-art cross-modal hashing methods."
SS2,Visible-infrared Person Re-identification with Human Body Parts Assistance,"Huangpeng Dai, Qing Xie, Jiachen Li, Yanchun Ma, Lin Li and Yongjian Liu","Wuhan University of Technology, Wuhan University of Technology, Wuhan University of Technology, Wuhan University of Technology, Wuhan University of Technology, Wuhan University of Technology","Person re-identification (re-id) has received ever-increasing research focus, because of its important role in video surveillance applications. This paper addresses the re-id problem between visible images of color cameras and infrared images of infrared cameras, which is significant in case that the appearance information is insufficient in poor illumination conditions. In this field, there are two key challenges, i.e., the difficulty to locate the discriminative information to re-identify the same person between visible and infrared images, and the difficulty to learn a robust metric for such large-scale cross-modality retrieval. In this paper, we propose a novel human body parts assistance network (BANet) to tackle the two challenges above. BANet mainly focuses on extracting discriminative information and learning robust features by leveraging the human body part cues. Extensive experiments demonstrate that the proposed approach outperforms the baseline and the state-of-the-art methods."
SS2,Look Back Again: Dual Parallel Attention Network for Accurate and Robust Scene Text Recognition,"Zilong Fu, Hongtao Xie, Guoqing Jin and Junbo Guo","University of Science and Technology of China, State Key Laboratory of Communication Content Cognition, People's Daily Online, Beijing, China 100733","Nowadays, it is a trend that using a parallel-decoupled encoder-decoder (PDED) framework in scene text recognition for its flexibility and efficiency. However, due to the inconsistent information content between queries and keys in the parallel positional attention module (PPAM) used in this kind of framework(queries: position information, keys: context and position information), visual misalignment tends to appear when confronting hard samples(e.g., blurred texts, irregular texts, or low-quality text images). To tackle this issue, in this paper, we propose a dual parallel attention network (DPAN), in which a newly designed parallel context attention module (PCAM) is cascaded with the original PPAM, using linguistic contextual information to compensate for the information inconsistency between queries and keys. Specifically, in PCAM, we take the visual features from PPAM as inputs and present a bidirectional language model to enhance them with linguistic semantic contexts to produce queries. In this way, we make the information content of the queries and keys consistent in PCAM, which helps generate more precise visual glimpses to improve the entire PDED framework’s accuracy and robustness. Experimental results verified the effectiveness of the proposed PCAM, showing the necessity of keeping the information consistency between queries and keys in the attention mechanism. On six benchmarks, including regular text and irregular text, the performance of DPAN surpasses the existing leading methods by large margins (e.g. 4.1% on CUTE80 and 3.9% on SVTP), achieving new state-of-the-art results.  The code will be available."
Oral 7,Efficient Nearest Neighbor Search by Removing Anti-hub,"Kimihiro Tanaka, Yusuke Matsui and Shin'Ichi Satoh","The University of Tokyo, The University of Tokyo, National Institute of Informatics","The central research question of the nearest neighbor search is how to reduce the memory cost while maintaining its accuracy. Instead of compressing each vector as is done in the existing methods, we propose a way to subsample unnecessary vectors to save memory.  We empirically found that such unnecessary vectors have low hubness scores and thus can be easily identified beforehand.  Such points are called anti-hubs in the data mining community. By removing anti-hubs, we achieved a memory-efficient search while preserving accuracy.  In million-scale experiments, we showed that any vector compression method improves search accuracy by partial replacement with anti-hub removal under the same memory usage. A billion-scale benchmark showed that our data reduction combined with the best search method achieves higher accuracy under the assumption of fixed memory consumption.  For example, our method had a much higher recall@100 (0.53) compared with the existing method (0.23) for the same memory consumption (6GB)."
Oral 7,Impact of Interaction Strategies on User Relevance Feedback,"Omar Shahbaz Khan, Björn Þór Jónsson, Jan Zahálka, Stevan Rudinac and Marcel Worring","IT University of Copenhagen, IT University of Copenhagen, Czech Technical University in Prague, University of Amsterdam, University of Amsterdam","User Relevance Feedback~(URF) is a class of interactive learning methods that rely on the interaction between a human user and a system to analyze a media collection. To improve URF system evaluation and design better systems, it is important to understand the impact that different interaction strategies can have.Based on the literature and observations from real user sessions from the Lifelog Search Challenge and Video Browser Showdown, we analyze interaction strategies related to labeling positive and negative examples as well as applying filters based on users' domain knowledge. Experiments show that there is no single optimal labeling strategy, as the best strategy depends on both the collection and the task.In particular, our results refute the common assumption that providing more training examples is always beneficial: strategies with a smaller number of prototypicalexamples lead to better results in some cases. We further observe that while expert filtering is unsurprisingly beneficial, aggressive filtering, especially by novice users, can hinder the completion of tasks. Finally, combining URF with filters leads to better results than using filters alone."
Oral 7,Text-Guided Visual Feature Refinement for Text-Based Person Search,"Liying Gao, Kai Niu, Zehong Ma, Bingliang Jiao, Tonghao Tan and Peng Wang","Northwestern Polytechnical University, Northwestern Polytechnical University, Northwestern Polytechnical University, Northwestern Polytechnical University, Northwestern Polytechnical University, Northwestern Polytechnical University","Text-based person search is a task to retrieve the corresponding person in a large-scale image database given a textual description, which has important value in various fields like video surveillance. In the inferring phase, language descriptions, serving as queries, guide to search the corresponding person images. Most existing methods apply cross-modal signals to guide feature refinement. However, they employ visual features from the gallery to refine textual (query) features, which we argue is an unreasonable guiding direction. Besides, the similarity-based cross-modal attention could disturb the choice of interested areas for descriptions. In this paper, we analyze the deficiency of previous methods and carefully design a Text-guided Visual Feature Refinement network (dubbed as TVFR), which utilizes text as reference to refine visual representations. Firstly, we divide each visual feature into several stripes horizontally for fine-grained refinement. After that, we employ a text-based filter generation module to generate description-customized filters, which are used to indicate the corresponding stripes mentioned in the textual input. Thereafter, we employ a text-guided visual feature refinement module to fuse part-level visual features adaptively for each description. In experiments, we validate our TVFR through extensive experiments on CUHK-PEDES, which is the only available dataset for text-based person search. To the best of our knowledge, the TVFR outperforms other state-of-the-art methods."
Oral 7,Ten Questions in Lifelog Mining and Information Recall,"An-Zi Yen, Hen-Hsen Huang and Hsin-Hsi Chen","National Taiwan University, Academia Sinica & MOST Joint Research Center for AI Technology and All Vista Healthcare, National Taiwan University & MOST Joint Research Center for AI Technology and All Vista Healthcare, ","With the advance of science and technology, people are used to recording their daily life events via writing blogs, uploading social media posts, taking photos, or filming videos. Such rich repository personal information is useful for supporting human living assistance, such as information recall service. The main challenges are how to store and manage personal knowledge from various sources, and how to provide support for people who may have difficulty recalling past experiences. In this position paper, we propose a research agenda on personal knowledge mining from various sources of lifelogs, personal knowledge base construction, and information recall for assisting people to recall their experiences. Ten research questions are formulated."
Oral 7,NASTER: Non-local Attentional Scene Text Recognizer,"Lei Wu, Xueliang Liu, Yanbin Hao, Richang Hong and Yunjie Ma","Hefei University of Technology, Hefei University of Technology, Hefei University of Technology, Hefei University of Technology, Hefei University of Technology","Scene text recognition has been widely investigated in computer vision. In the literature, the encoder-decoder based framework, which first encodes image into feature map and then decodes them into corresponding text sequences, have achieved great success. However, this solution fails in low-quality images, as the local visual features extracted from curved or blurred images are difficult to decode into corresponding text. To address this issue, we propose a new framework for Scene Text Recognition (STR), named Non-Local Attentional Scene Text Recognizer (NASTER). We use ResNet with Global Context Block (GC block) to extract global visual features. The global context information is then captured in parallel using the self-attention module and finally decoded by a multi-layer attention decoder with an intermediate supervision module. The proposed method achieves the state-of-the-art performances on seven benchmark datasets, demonstrating the effectiveness of our approach."
Oral 8,Relation-Aware Keypoint Matching with Space-Spanned Feature,"Jingwei Qu, Haibin Ling, Tianle Wang, Xiaoqing Lv and Zhi Tang","Wangxuan Institute of Computer Technology, Peking University, Stony Brook University, Beijing University of Posts and Telecommunications, Wangxuan Institute of Computer Technology, Peking University","Deriving from image matching, keypoint matching aims at establishing correspondence between keypoint sets. Graph matching provides an effective way to find desired keypoint correspondences with the ability of graph for representing non-Euclidean data by its nodes and edges. Edges between keypoints are usually constructed by heuristic ways. These edges describe the spatial relations between keypoints, but their features depend heavily on associated node features and lack intrinsic descriptions. Besides, noisy edge correspondences may exist in graph matching then interfere positive matching. However, the analysis for impact of different edge correspondences on graph matching is insufficient. To address these issues, we propose an end-to-end Relation-Aware Graph Matching (RAGM) network, coupled with a novel description of edge features. RAGM transforms the matching between two graphs into a node and edge classification problem over their assignment graph. To explore the relation between edges from the two graphs respectively, RAGM learns a relation-aware attention to adaptively adjust the impact of each edge correspondence on graph matching. To alleviate the lack of intrinsic edge features, we design a space-spanned feature for each edge by aggregating the semantic information over the space spanned by it. This feature provides informative and independent descriptions for edges, which clearly distinguishes different edges (e.g., inlier-inlier edges vs. inlier-outlier edges). Besides, the feature also supports discriminating outliers by the view of their associated edges. Extensive experiments demonstrate that RAGM achieves promising matching quality compared with state-of-the-arts on different matching cases."
Oral 8,Learning Hierarchical Visual-Semantic Representation with Phrase Alignment,"Baoming Yan, Qingheng Zhang, Liyu Chen, Lin Wang, Leihao Pei, Jiang Yang, Enyun Yu, Xiaobo Li and Binqiang Zhao","Alibaba Group, Alibaba Group, Alibaba Group, Alibaba Group, Alibaba Group, Alibaba Group, Alibaba Group, Alibaba Group, Alibaba Group","Effective visual-semantic representation is critical to the image-text matching task. Various methods are proposed to develop image representation with more semantic concepts and a lot of progress has been achieved. However, the internal hierarchical structure in both image and text, which could effectively enhance the semantic representation, is rarely explored in the image-text matching task. In this work, we propose a Hierarchical Visual-Semantic Network (HVSN) with fine-grained semantic alignment to exploit the hierarchical structure. Specifically, we first model the spatial or semantic relationship between objects and aggregate them into visual semantic concepts by the Local Relational Attention (LRA) module. Then we employ gate and memory mechanisms to learn relationships between visual semantic concepts and generate the global image representation. For the text part, we develop phrase features from related words, then generate text representation by learning relationships between these phrases. Besides, the model is trained with joint optimization of image-text retrieval and phrase alignment task to capture the fine-grained interplay between vision and language.  Our approach achieves state-of-the-art performance on Flickr30K and MS-COCO datasets. On Flickr30K, our approach outperforms the current state-of-the-art method by 3.9\% relatively in text retrieval with image query and 1.3\% relatively for image retrieval with text query (based on Recall@1). On MS-COCO, our HVSN improves image retrieval by 2.3\% relatively and text retrieval by 1.2\% relatively. Both quantitative and visual ablation studies are provided to verify the effectiveness of the proposed modules."
Oral 8,A Unified-Model via Block Coordinate Descent for Learning the Importance of Filter,"Qinghua Li, Xue Zhang, Cuiping Li and Hong Chen","Renmin University of China, Renmin University of China, Renmin University of China","Deep Convolutional Neural Networks (CNNs) are increasingly used in multimedia retrieval, and accelerating Deep CNNs has recently received an ever-increasing research focus. Among various ap- proaches proposed in the literature, filter pruning has been regarded as a promising solution, which is due to its advantage in signifi- cant speedup and memory reduction of both network model and intermediate feature maps. Many works have been proposed to find unimportant filters, and then prune it for accelerating Deep CNNs. However, they mainly focus on using heuristic methods to evaluate the importance of filters, such as the statistical informa- tion of filters (e.g., prune filter with small l2-norm), which may be not perfect. In this paper, we propose a novel filter pruning method, namely A Unified-Model via Block Coordinate Descent for learning the importance of filter (U-BCD). The importance of the filters in our U-BCD is learned by optimizing method. We can simultaneously learn the filter parameters and the importance of filters by block coordinate descent method. When applied to two image classification benchmarks, the effectiveness of our U-BCD is validated. Notably, on CIFAR-10, our U-BCD reduces more than 57% FLOPs on ResNet-110 with even 0.08% relative accuracy im- provement, and also achieve state-of-the-art results on ILSVRC- 2012. Code is available at : https://drive.google.com/drive/folders/ 13gY7blcaSRAo9NpQxCIkO6IVYOCQzo_3?usp=sharing"
Oral 8,Neural Symbolic Representation Learning for Image Captioning,"Xiaomei Wang, Yanwei Fu, Lin Ma and Xiangyang Xue","Fudan University, Fudan University, Fudan University","Traditional image captioning models mainly rely on one encoder-decoder architecture to generate one  natural sentence for a given image. Such an architecture mostly uses deep neural networks to extract the  neural representations of the image while ignoring the information of abstractive concepts as well as their intertwined relationships conveyed in the image. To this end, to comprehensively characterize the image content and bridge the gap between neural representations and high-level abstractive concepts, we make the first attempt to investigate the ability of neural symbolic representation of the image for the image captioning task.  We first parse and convert a given image to neural symbolic representation in the form of an attributed relational graph, with the nodes denoting the abstractive concepts and the branches indicating the relationships between connected nodes, respectively. By performing computations over the attributed relational graph, the neural symbolic representation evolves step by step, with the node and branch representations as well as their corresponding importance weights transiting step by step.   Empirically,  extensive experiments validate the effectiveness of the proposed method. It enables a more comprehensive understanding of the given image by integrating  the neural representation and neural symbolic representation, with  the state-of-the-art results being achieved on both the MSCOCO and Flickr30k Entities datasets. Besides, the proposed neural symbolic representation is demonstrated to better generalize to other domains with significant performance improvements compared with existing methods on the cross domain image captioning task."
Oral 9,GPT2MVS: Generative Pre-trained Transformer-2 for Multi-modal Video Summarization,"Jia-Hong Huang, Luka Murn, Marta Mrak and Marcel Worring","University of Amsterdam, BBC Research and Development & Dublin City University,  BBC Research and Development, University of Amsterdam","Traditional video summarization methods generate fixed video representations regardless of user interest. Therefore such methods limit users' expectations in content search and exploration scenarios. Multi-modal video summarization is one of the methods utilized to address this problem. When multi-modal video summarization is used to help video exploration, a text-based query is considered as one of the main drivers of video summary generation, as it is user-defined. Thus, encoding the text-based query and the video effectively are both important for the task of multi-modal video summarization. In this work, a new method is proposed that uses a specialized attention network and contextualized word representations to tackle this task. The proposed model consists of a contextualized video summary controller, multi-modal attention mechanisms, an interactive attention network, and a video summary generator. Based on the evaluation of the existing multi-modal video summarization benchmark, experimental results show that the proposed model is effective with the increase of +5.88% in accuracy and +4.06% increase of F1-score, compared with the state-of-the-art method."
Oral 9,Local-enhanced Interaction for Temporal Moment Localization,"Guoqiang Liang, Shiyu Ji and Yanning Zhang","Northwestern Polytechnical University, Northwestern Polytechnical University, Northwestern Polytechnical University","Temporal moment localization via language aims to localize a video span in an untrimmed video which best matches the given natural language query. In most previous works, they try to match the whole query feature with multiple moment proposals, or match a global video embedding with phrase or word level query features. However, these coarse interaction models will become insufficient when the query-video contains more complex relationship. To address this issue, we propose a multi-branches interaction model for temporal moment localization. Specifically, the query sentence and video are encoded into multiple feature embeddings over several semantic sub-spaces. Then, each phrase embedding filters on a video feature to generate an attention sequence, which is used to re-weight the video features. Moreover, a dynamic pointer decoder is developed to iteratively regress the temporal boundary, which can prevent our model from falling into a local optimum. To validate the proposed method, we have conducted extensive experiments on two popular benchmark datasets Charade-STA and TACoS. The experimental performance surpasses other state-of-the-arts methods, which demonstrates the effectiveness of our proposed model."
Oral 9,Multi-Attention Audio-Visual Fusion Network for Audio Spatialization,Wen Zhang and Jie Shao,"University of Electronic Science and Technology of China, Sichuan Artificial Intelligence Research Institute","In our daily life, we are exposed to a large number of video files. Compared with video containing only mono audio, video with stereo can provide us with a richer audio-visual experience. However, a large number of ordinary users do not have professional equipment to record videos with high-quality stereo. In order to make it more convenient for users to obtain videos with stereo, we propose an effective method to convert mono audio in the video into stereo. One of the keys to this task is how to effectively inject visual information extracted from video frames into the audio signal. We design a novel multi-attention fusion network (MAFNet) based on the self-attention mechanism to extract the spatial features related to the sound source in the video frames and fuse them into audio features well. Furthermore, in order to obtain stereo with higher quality, we design an additional iterative structure which can refine and optimize the generated stereo sound by several iterations. Our proposed approach is validated on two challenging video datasets (FAIR-Play and YT-MUSIC), and achieves new state-of-the-art performance."
Oral 9,MS-SincResNet: Joint Learning of 1D and 2D Kernels Using Multi-scale SincNet and ResNet for Music Genre Classification,"Pei-Chun Chang, Yong-Sheng Chen and Chang-Hsing Lee","National Yang Ming Chiao Tung University,National Yang Ming Chiao Tung University, Chung Hua University","In this study, we proposed a new end-to-end convolutional neural network, called MS-SincResNet, for music genre classification. MS-SincResNet appends 1D multi-scale SincNet (MS-SincNet) to 2D ResNet as the first convolutional layer in an attempt to jointly learn 1D kernel and 2D kernel during the training stage. First, an input music signal is divided into a number of fixed-duration (3 seconds in this study) music clips, and the raw waveform of each music clip is fed into 1D MS-SincNet filter learning module to obtain three-channel 2D representations. The learned representations carry rich timbre and harmonic characteristic comparing with Mel-spectrograms. ResNet is then used to extract discriminative embeddings from these 2D representations. The spatial pyramid pooling (SPP) module is further used to enhance the feature discriminability in terms of both time and frequency aspects. Finally, the voting strategy is applied to summarize the classification results from all 3-second music clips. In our experimental results, we demonstrate that the proposed MS-SincResNet outperforms the baseline SincNet and many well-known hand-craft features. Considering individual 2D representation, we also yield the competitive results with the state-of-the-art methods on the GTZAN dataset and the ISMIR2004 dataset."
Oral 10,Can Action be Imitated? Learn to Reconstruct and Transfer Human Dynamics from Videos,"Yuqian Fu, Yanwei Fu and Yu-Gang Jiang","Fudan University, Fudan University, Fudan University","Given a video demonstration, can we imitate the action contained in this video? In this paper, we introduce a novel task, dubbed mesh-based action imitation. The goal of this task is to enable an arbitrary target human mesh to perform the same action shown on the video demonstration. To achieve this, a novel Mesh-based Video Action Imitation (M-VAI) method is proposed by us. M-VAI first learns to reconstruct the meshes from the given source image frames, then the initial recovered mesh sequence is fed into mesh2mesh, a mesh sequence smooth module proposed by us, to improve the temporal consistency. Finally, we imitate the actions by transferring the pose from the constructed human body to our target identity mesh. High-quality and detailed human body meshes can be generated by using our M-VAI. Extensive experiments demonstrate the feasibility of our task and the effectiveness of our proposed method."
Oral 10,HPOF:3D Human Pose Recovery from Monocular Video with Optical Flow,"Bin Ji, Ye Pan, Chen Yang and Yao Shunyu","Shanghai Jiao Tong University, Shanghai Jiao Tong University, Shanghai Jiao Tong University","In this paper, we introduce HPOF, a novel deep neural network to reconstruct 3D motion of human body from monocular video. Recently, model-based methods have been proposed to simplify the reconstruction task by estimating several parameters to fit a deformable surface model to the person in the image. However, learning the parameters from a single image is a highly ill-posed problem, and the process is ultimately data-driven. Existing 3D datasets are not sufficient enough and the usage of 2D in-the-wild datasets often suffers from the poor precision of 2D manual annotations. To address above issues, our method yields substantial improvements in two domain. First, to gain more high quality datasets, we leverages optical flow as the plausible and inexpensive 2D supervision information. Besides, our HPOF takes temporal consistency into account in order to make full use of existing data. At the crux of our method lies a dilated temporal convolutional network that embeds the anthropometric and kinematic priors like shape consistency and pose continuity. What is more, an adversarial learning framework is applied to supervise the reconstruction progress from a comprehensive perspective. We show that HPOF not only improves the accuracy of 3D poses but ensures the realistic body structure throughout the video. We perform extensive experimentation to demonstrate the superiority of our method over the traditional motion capture method and analyze the effectiveness of our model, surpassing other state-of-the-art deep neural networks performance."
Oral 10,Multi-scale Dynamic Network for Temporal Action Detection,"Yifan Ren, Xing Xu, Fumin Shen, Zheng Wang, Yang Yang and Heng Tao Shen","University of Electronic Science and Technology of China, University of Electronic Science and Technology of China,  University of Electronic Science and Technology of China, University of Electronic Science and Technology of China, University of Electronic Science and Technology of China","In recent years, as the fundamental task in video understanding, \textit{Temporal Action Detection} is attracting extensive attention. Most existing approaches use the same model parameters to process all input videos, which are not adaptive to the input video during the inference stage.  In this paper, we propose a novel model termed \textit{Multi-scale Dynamic Network} (MDN) to tackle this problem. The proposed MDN model incorporates multiple Multi-scale Dynamic Modules (MDMs).  Each MDM can generate video-specific and segment-specific convolution kernels based on video content from different scales and adaptively capture rich semantic information for the prediction. Besides, we also design a new Edge Suppression Loss (ESL) function for MDN to pay more attention to hard examples. Extensive experiments conducted on two popular benchmarks ActivityNet-1.3 and THUMOS-14 show that the proposed MDN model achieves state-of-the-art performance."
Oral 10,SAGN: Semantic Adaptive Graph Network for Skeleton-Based Human Action Recognition,"	Ziwang Fu, Feng Liu, Jiahao Zhang, Hanyang Wang, Chengyi Yang, Qing Xu, Jiayin Qi, Xiangling Fu, Aimin Zhou and Zhibin Li","Beijing University of Posts and Telecommunications, East China Normal University, East China Normal University, East China Normal University, Shanghai University of International Business and Economics, Beijing University of Posts and Telecommunications, Shanghai University of International Business and Economics, Beijing University of Posts and Telecommunications","With the continuous development and popularity of depth cameras, skeleton-based human action recognition has attracted people's wide attention. Graph Convolutional Network (GCN) has achieved remarkable performance. However, the existing methods do not better consider the semantic characteristics, which can help to express the current concept and scene information. Semantic information can also help with better granularity classification. In addition, most of the existing models require a lot of computation. What’s more, adaptive GCN can automatically learn the graph structure and consider the connections between joints. In this paper, we propose a relatively less computationally intensive model, which combines semantic and adaptive graph network (SAGN) for skeleton-based human action recognition. Specifically, we mainly combine the dynamic characteristics and bone information to extract the data, taking the correlation between semantics into the model. In the training process, SAGN includes an adaptive network so that we can make attention mechanism more flexible. And we design the Convolutional Neural Network (CNN) for feature extraction on the time dimension. The experimental results show that SAGN achieves the state-of-the-art performance on NTU-RGB+D 60 and NTU-RGB+D 120 datasets. SAGN can promote the study of skeleton-based human action recognition. The source code is available at https://github.com/buptSAGN/SAG."
Oral 10,Joint Hand-Object Pose Estimation with Differentiably-Learned Physical Contact Point Analysis,Nan Zhuang and Yadong Mu,"Peking University, Peking University","Hand-object pose estimation aims to jointly estimate the 3D posesof hands and held objects. During the interaction between handsand objects, the position and motion of keypoints in hands andobjects are tightly related and there naturally exist some physicalrestrictions, which is usually ignored by most previous methods.To address this issue, we propose a learnable physical constraintloss to regularize the joint estimation of hand and object poses.The physical constraints mainly focus on enhancing the stabilityof grasp, which is the most common interaction manner betweenhands and objects. Our key insight is, for a stable grasp, the contactpoints between hands and objects should distribute around the objectcenter axis, so that the center of the manipulated object can bein affinity with the center of hand to make it robust against externalforces and disturbances. Together with the physical constraint loss,a context-aware graph network is also proposed to jointly learnindependent geometry prior and interaction messages. The wholepipeline consists of two components. First an image encoder is usedto predict 2D keypoints from RGB image and then a contextualgraph module is designed to convert 2D keypoints into 3D estimations.Our graph module treats the keypoints of hands and objectsas two sub-graphs and estimates initial 3D coordinates accordingto their topology structure separately. Then the two sub-graphs aremerged into a whole graph to capture the interaction informationand further refine the 3D estimation results. Experimental resultsshow that both our physical constraint loss and our context-awaregraph network can effectively capture the relationship and improvethe accuracy of 3D pose estimation."
CEA,Few-Shot and Zero-Shot Semantic Segmentation for Food Images,Yuma Honbu and Keiji Yanai,"The University of Electro-Communications, The University of Electro-Communications","With the popularity of health management applications, awareness of dietary management is increasing. When calculating the amount of calories in a dish, discriminating between food regions is an important factor. However, when using deep learning, a large amount of data is required for training, and it is impractical to collect data for countless food categories. In recent years, a method called few-shot segmentation has been studied to learn a semantic segmentation model using a small amount of training data. In this study, we propose a few-shot and zero-shot segmentation model which targets food images to overcome the insufficient amount of food training data, and show the effectiveness of the proposed model on a semantic segmentation task for new food classes. In the proposed model, we employed word embedding, which results in better accuracy than the previous methods."
CEA,World Food Atlas Project,"Ali Rostami, Zhouhang Xie, Akihisa Ishino, Yoko Yamakata, Kiyoharu Aizawa and Ramesh Jain","The University of Irvine, The University of Irvine, The University of Irvine, The University of Irvine, The University of Irvine, The University of Irvine, The University of Irvine","A coronavirus pandemic is forcing people to be ""at home"" all over the world. In a life of hardly ever going out, we would have realized how the food we eat affects our bodies. What can we do to know our food more and control it better? To give us a clue, we are trying to build a World Food Atlas (WFA) that collects all the knowledge about food in the world. In this paper, we present two of our trials. The first is the Food Knowledge Graph (FKG), which is a graphical representation of knowledge about food and ingredient relationships derived from recipes and food nutrition data. The second is the FoodLog Athl and the RecipeLog that are applications for collecting people's detailed records about food habit. We also discuss several problems that we try to solve to build the WFA by integrating these two ideas."
CEA,Region-Based Food Calorie Estimation for Multiple-Dish Meals,"Kaimu Okamoto, Kento Adachi and Keiji Yanai","The University of Electro-Communications, Tokyo, The University of Electro-Communications, Tokyo, The University of Electro-Communications, Tokyo","One of the major tasks in food computing is vision-based food calorie estimation.  However, unfortunately, food image datasets annotated with calorie amounts  are very hard to obtain.  In fact, no large-scale food datasets annotated with calorie amounts  exits as long as we know. However, we can see some Web sites which provide photos of food set menus with only total calorie values. Then, in this work,  we crawl such data from the Web and use them as training data  of a vision-based food calorie estimation model. To estimate calorie amounts of food items,  the calorie values of each food item in meal photos should be known in general. However, they are not available in this setting. Then, we propose a model employing food segmentation which  can estimate calorie amounts of each food item from total calorie values of set meal photos. The experimental results showed that  our region-segmentation-based calorie estimation model was able to estimate calorie amounts of individual food items roughly."
CEA,Critique Generation to Increase Diversity in Conversational Recipe Recommender System,"Fakhri Abbas, Nadia Najjar and David Wilson","University of North Carolina at Charlotte, University of North Carolina at Charlotte, University of North Carolina at Charlotte","Conversational recommender systems help to guide users in exploring the search space in order to discover items of interest. During the exploration process, the user provides feedback on recommended items to refine subsequent recommendations. Critiquing as a way of feedback has proven effective for conversational interactions. In addition, diversifying the recommended items during exploration can help increase user understanding of the search space, which critiquing alone may not achieve. Both aspects are important elements for recommender applications in the food domain. Diversity in diet has been shown to predict nutritional health, and conversational exploration can help to introduce new food items. In this paper, we introduce a novel approach that brings together critique and diversity to support conversational recommendation in the recipe domain. Initial evaluation in comparison to a baseline similarity-based recommender shows that the proposed approach increases diversity during the exploration process."
CEA,Boosting Personalized Food Image Classifier by Sharing Food Records,"Seum Kim, Yoko Yamakata and Kiyoharu Aizawa","The University of Tokyo, The University of Tokyo, The University of Tokyo","Food image recognition tasks are generally addressed by using a closed dataset. In a real-world setting, however, the dataset is updated as the new class of food appears, and it is impossible to train a model that distinguishes all kinds of food in advance. Also, inter-class similarity and intra-class diversity make food image recognition tasks more challenging. Previous works have shown that a personalized classifier using an individual user's food records can deal with some of these problems. On top of that, we propose a personalized classifier that learns from not only the accumulating food records of the individual user but also the growing records of the entire user. To conduct a realistic experiment, we build a new dataset of daily food images using a food-logging application called FoodLog Athl. As a result, our proposed method significantly outperforms prior personalized classification methods for food image recognition in a realistic setting."
CEA,IYASHI Recipe: Cooking Recipe Recommendation for Healing based on Physical Conditions and Human Relations,"Takuya Yonezawa, Shion Yamaguchi, Yuanyuan Wang, Kazutoshi Sumiya and Yukiko Kawai","Kyoto Sangyo University, Kyoto Sangyo University, Kyoto Sangyo University","In this paper, we propose a recipe recommendation method that can heal both the provider and the recipient of food. In particular, we focus on the motivation of recipe contributors, and propose a recipe recommendation method that can heal both parties by considering not only the recipe procedure but also the relationship between the provider and the recipient, and their health conditions. Specifically, we extract feature words by learning from the items in the recipe data that contain a lot of information other than the cooking procedure, such as ""title,"" ""introduction of the recipe,"" ""one- point information,"" and ""trigger. In addition, each item of the recipe is analyzed for sentiment, and ranked based on the extracted feature words and sentiment values of physical condition and relationship. In this paper, we validate the usefulness of the proposed method by evaluating the relationship and physical condition feature words extracted by the proposed method and by evaluating the recipe ranking using Rakuten’s recipe data."
ICDAR,ST-HOI: A Spatial-Temporal Baseline for Human-Object Interaction Detection in Videos,"Meng-Jiun Chiou, Chun-Yu Liao, Li-Wei Wang, Roger Zimmermann and Jiashi Feng","National University of Singapore, ASUS Intelligent Cloud Services, National University of Singapore, National University of Singapore","Detecting human-object interactions (HOI) is an important step toward a comprehensive visual understanding of machines. While detecting non-temporal HOIs (e.g., sitting on a chair) from static images is feasible, it is unlikely even for humans to guess temporal-related HOIs (e.g., opening/closing a door) from a single video frame, where the neighboring frames play an essential role. However, conventional HOI methods operating on only static images have been used to predict temporal-related interactions, which is essentially guessing without temporal contexts and may lead to sub-optimal performance. In this paper, we bridge this gap by detecting video-based HOIs with explicit temporal information. We first show that a naive temporal-aware variant of a common action detection baseline does not work on video-based HOIs due to a feature-inconsistency issue. We then propose a simple yet effective architecture named Spatial-Temporal HOI Detection (ST-HOI) utilizing temporal information such as human and object trajectories, correctly-localized visual features, and spatial-temporal masking pose features. We construct a new video HOI benchmark dubbed VidHOI where our proposed approach serves as a solid baseline. The source code and the benchmark will be available soon."
ICDAR,Scattering Transform Based Image Clustering using Projection onto Orthogonal Complement,Angel Villar-Corrales and Veniamin Morgenshtern,"University of Bonn, Friedrich-Alexander-Universität Erlangen-Nürnberg (FAU) ","In the last few years, large improvements in image clustering have been driven by the recent advances in deep learning. However, due to the architectural complexity of deep neural networks, there is no mathematical theory that explains the success of deep clustering techniques. In this work we introduce Projected-Scattering Spec- tral Clustering (PSSC), a state-of-the-art, stable, and fast algorithm for image clustering, which is also mathematically interpretable. PSSC includes a novel method to exploit the geometric structure of the scattering transform of small images. This method is inspired by the observation that, in the scattering transform domain, the subspaces formed by the eigenvectors corresponding to the few largest eigenvalues of the data matrices of individual classes are nearly shared among different classes. Therefore, projecting out those shared subspaces reduces the intra-class variability, substan- tially increasing the clustering performance. We call this method ‘Projection onto Orthogonal Complement’ (POC). Our experiments demonstrate that PSSC obtains the best results among all shallow clustering algorithms. Moreover, it achieves comparable clustering performance to that of recent state-of-the-art clustering techniques, while reducing the execution time by more than one order of mag- nitude. In the spirit of reproducible research, we publish a high quality code repository along with the paper."
ICDAR,Temperature Forecasting using Tower Networks,"Siri Sofie Eide, Michael Riegler, Hugo Hammer and John Bjørnar Bremnes","Norwegian Meteorological Institute, Simula Research Laboratory, OsloMet - Oslo Metropolitan University","In this paper, we present the tower network, a computationally lightweight deep neural network for video prediction. The tower network is especially useful when it comes to combining different types of input data, a problem not greatly explored within deep learning.The architecture is further applied to a real-world example, where information from historic meteorological observations and numerical weather predictions are combined to produce high-quality forecasts of temperature."
ICDAR,Pyramidal Segmentation of Medical Images via Generative Adversarial Networks,"Espen Næss, Vajira Thambawita, Steven Hicks, Michael Riegler and Pål Halvorsen","Simula Research Laboratory, Simula Research Laboratory, Simula Research Laboratory, Simula Research Laboratory, Simula Research Laboratory","Colorectal cancer is a severe health issue globally and a significant cause of cancer-related mortality, but it is treatable if found at an early stage. Early detection is usually done through a colonoscopy, where clinicians search for cancer precursors called polyps. Research has shown that clinicians miss between 14% and 30% of polyps during standard screenings of the gastrointestinal tract. Furthermore, once the polyps have been found, clinicians often overestimate the size of the polyps. In this respect, automatic analysis of medical videos for detecting and locating polyps is a research area where machine learning has excelled in recent years. Still, current models have much room for improvement. In this paper, we propose a novel approach based on learning to segment within several grids, which we introduce to U-Net and Pix2Pix architectures. In short, we have experimented using several grid sizes, and using two open-source polyp segmentation datasets for cross-data training and testing. Our results suggest that segmentation at lower resolutions produces better results at the cost of less precision, which proved useful for the cases where higher precision segmentations gave limited results. Generally, compared to traditional U-Net and Pix2Pix, our grid-based approaches improve segmentation performance."
ICDAR,Cross-Modal Deep Neural Networks based Smartphone Authentication for Intelligent Things System,"Khoa Anh Tran, Truong The Truong Nguyen and Duc Minh Ngoc Duc","Ton Duc Thang University, Ton Duc Thang University, Ton Duc Thang University","Nowadays, identity authentication technologies play an essential role in the safety of smart devices such as biological features such as the iris and the fingerprint for identity authentication. However, it cannot implement real-time and continuous identification of user identity. Here, we present a user authentication framework from smartphone-acquired motion signals. This research aims to recognize a target user from their way of walking, using the accelerometer and gyroscope signals, which is provided by a commercial smartphone worn in the front pocket of the user's trousers. The proposed innovation scheme including i) a data preprocessing, ii) a novel feature extraction and authentication scheme based on a cross-modal deep neural network by applying a Convolutional Neural Network (CNN), and time-distributed Long Short-Term Memory (LSTM). The experimental results of the proposed scheme show the superiority of our approach against state-of-the-art techniques."
ICDAR,Dutkat: A Multimedia System for Catching Illegal Catchers in a Privacy-Preserving Manner,"Tor-Arne Nordmo, Aril Ovesen, Håvard Johansen, Michael Riegler, Pål Halvorsen and Dag Johansen","UiT The Arctic University of Norway, UiT The Arctic University of Norway, UiT The Arctic University of Norway, SimulaMet, SimulaMet, UiT The Arctic University of Norway","Fish crime is considered a global and serious problem for a healthy and sustainable development of one of mankind’s important sources of food. Technological surveillance and control solutions are emerging as remedies to combat such criminal activities, but such solutions might also come with impractical and negative side-effects and challenges. In this paper, we present the prototype of a surveillance system in lieu of current surveillance trends striking a delicate balance between privacy of legal actors while simultaneously capturing evidence-based footage, sensory data, and forensic proofs of illicit activities. Our novel approach is to assist human operators in the 24/7 surveillance loop of remote professional fishing activities with a privacy-preserving Artificial Intelligence (AI) surveillance system operating in the same proximity as the activities being surveyed. The system is primarily using video surveillance data but also other sensor data captured on the fishing vessel.  Additionally, the system correlates with other sources such as reports from other fish catches in the approximate area and time, etc. Only upon true positive flagging of specific potentially illicit activities by the locally executing AI algorithms, can forensic evidence be accessed from this physical edge, the fishing vessel. Besides a more privacy-preserving solution, our edge-based AI system also benefits from much less data that has to be transferred over unreliable, low-bandwidth networks."
ICDAR,Investigation on Privacy-Preserving Techniques For Personal Data,Rafik Hamza and Koji Zettsu,"National Institute of Information and Communications Technology, National Institute of Information and Communications Technology","Privacy protection technology has become a crucial part of almost every existing cross-data analysis application. The privacy-preserving data technique allows sharing sensitive personal information and preserves the users' privacy. This new trend influences data collection results by improving the analytic accuracy, increasing the number of participants, and better understand the participants' environments. Herein, collecting these personal data is significant to many advantageous applications such as health monitoring. Nevertheless, these applications encounter real privacy threats and concerns about handling personal information. This paper aims to determine privacy-preserving personal data mining technologies and analyze these technologies' advantages and shortcomings. We present a taxonomy for the existing personal data and security & privacy and then survey the most relevant privacy-preserving techniques from the literature. Moreover, we survey and elaborate on each type of privacy-preserving metric and requirement used to evaluate privacy-preserving algorithms. Our purpose is to provide an in-depth understanding of personal data privacy and highlight important viewpoints, existing challenges, and future research directions."
ICDAR,"Models to Predict Sleeping Quality from Activities and Environment: Current Status, Challenges and Opportunities","Thi Phuoc Van Nguyen, Do Van Nguyen and Koji Zettsu",National Institute of Information and Communications Technology,"The development of remote/wearable sensors enables more research in the health care area. Based on these kinds of sensors, the information of human’s active level, health parameters can be collected to predict their current health status. Sleeping quality is an important factor to make a person feel healthy. In this work, we summarize the current models to predict sleeping quality. Inputs of those models could be environmental factors, activities, or time-series data from wearable sensors. The characteristic of the input data may lead to the choice of predicting models. The domain of data that was used to forecast sleeping quality will be considered carefully in parallel with the predicted model. Moreover, we also discuss challenges and future work for this research direction."
ICDAR,Multimodal Virtual Avatars for Investigative Interviews with Children,"Gunn Astrid Baugerud, Miriam S. Johnson, Ragnhild Klingenberg Røed, Michael E. Lamb, Martine Powell, Vajira Thambawita, Pegah Salehi, Syed Zohaib Hassan, Michael Riegler and Pål Halvorsen","Oslo Metropolitan University, Oslo Metropolitan University, Oslo Metropolitan University, University of Cambridge, University of Griffith, ","In this article, we present our ongoing work in the field of training police officers who conduct interviews with abused children. The objectives in this context are to protect vulnerable children from abuse, facilitate prosecution of offenders, and ensure that innocent adults are not accused of criminal acts. There is therefore a need for more data that can be used for improved interviewer training to equip police with the skills to conduct high quality interviews. To support this important task, we propose to research a training program that utilizes different system components and multimodal data from the field of artificial intelligence such as chatbots, generation of visual content, text-to-speech, and speech-to-text. This program will be able to generate an almost unlimited amount of interview and also training data. The goal of combining all these different technologies and datatypes is to create an immersive and interactive child avatar that responds in a realistic way, to help to support training of police interviewers, but can also produce synthetic data of interview situations that can be used to solve different problems in the same domain."
ICDAR,Two-Faced Humans on Twitter and Facebook: Harvesting Social Multimedia for Human Personality Profiling,"Qi Yang, Aleksandr Farseev and Andrey Filchenkov","ITMO University, ITMO University, ITMO University","Human personality traits are the key drivers behind our decision-making, influencing our life path on a daily basis. Inference of personality traits, such as Myers-Briggs Personality Type, as well as an understanding of dependencies between personality traits and users' behavior on various social media platforms is of crucial importance to modern research and industry applications. The emergence of diverse and cross-purpose social media avenues makes it possible to perform user personality profiling automatically and efficiently based on data represented across multiple data modalities. However, the research efforts on personality profiling from multi-source multi-modal social media data are relatively sparse, and the level of impact of different social network data on machine learning performance has yet to be comprehensively evaluated. Furthermore, there is not such dataset in the research community to benchmark. This study is one of the first attempts towards bridging such an important research gap. Specifically, in this work, we infer the Myers-Briggs Personality Type indicators, by applying a novel multi-view fusion framework, called ""PERS"" and comparing the performance results not just across data modalities but also with respect to different social network data sources and release the multi-source multi-modal personality datasets. Our experimental results demonstrate the PERS's ability to learn from multi-view data for personality profiling by efficiently leveraging on the significantly different data arriving from diverse social multimedia sources. We have also found that the selection of a machine learning approach is of crucial importance when choosing social network data sources and that people tend to reveal multiple facets of their personality in different social media avenues. "
LSC,lifeXplore at the Lifelog Search Challenge 2021,Andreas Leibetseder and Klaus Schoeffmann,"Alpen-Adria-Universität Klagenfurt, Institute of Information Technology (ITEC), Klagenfurt University","Since its first iteration in 2018, the Lifelog Search Challenge (LSC) continues to rise in popularity as an interactive lifelog data retrieval competition, co-located at the ACM Conference on Multimedia Retrival (ICMR). The goal of this annual live event is to search a large corpus of lifelogging data for specifically announced memories using a purposefully developed tool within a limited amount of time. As long-standing participants, we present our improved lifeXplore -- a retrieval system combining chronologic day summary browsing with interactive combinable concept filtering. Compared to previous versions, the tool is improved by incorporating temporal queries, advanced day summary features as well as usability improvements."
LSC,Exploring Intuitive Lifelog Retrieval and Interaction Modes in Virtual Reality with vitrivr-VR,"Florian Spiess, Ralph Gasser, Silvan Heller, Luca Rossetto, Loris Sauter, Milan van Zanten and Heiko Schuldt","University of Basel, University of Basel, University of Basel, University of Zurich, University of Basel, University of Basel, University of Basel","The multimodal nature of lifelog data collections poses unique challenges for multimedia management and retrieval systems. The Lifelog Search Challenge (LSC) offers an annual evaluation platform for such interactive retrieval systems. They compete against one another in finding items of interest within a set time frame.  In this paper, we present the multimedia retrieval system vitrivr-VR, the latest addition to the vitrivr stack, which participated in the LSC in recent years. vitrivr-VR leverages the 3D space in virtual reality to offer novel retrieval and user interaction models, which we describe with a special focus on design decisions taken for the participation in the LSC."
LSC,Interactive Multimodal Lifelog Retrieval with vitrivr at LSC 2021,"Silvan Heller, Ralph Gasser, Sanja Popovic, Luca Rossetto, Loris Sauter, Florian Spiess, Heiko Schuldt and Mahnaz Parian","University of Basel, University of Zurich, Interactive Multimodal Lifelog Retrieval with vitrivr at LSC 2021, Alpen-Adria-Universität Klagenfurt, Alpen-Adria-Universität Klagenfurt","The Lifelog Search Challenge (LSC) is an annual benchmarking competition for interactive multimedia retrieval systems, where participating systems compete in finding events based on textual descriptions containing hints about structured, semi-structured, and/or unstructured data. In this paper, we present the multimedia retrieval system vitrivr, a long-time participant to LSC, with a focus on new functionality.  Specifically, we introduce the image stabilisation module which is added prior to the feature extraction to reduce the image degradation caused by lifelogger movements, and discuss how geodata is used during query formulation, query execution, and result presentation."
LSC,XQC at the Lifelog Search Challenge 2021: Interactive Learning on a Mobile Device,"Emil Knudsen, Thomas Holstein Qvortrup, Omar Shahbaz Khan and Björn Þór Jónsson","IT University of Copenhagen, IT University of Copenhagen, IT University of Copenhagen, IT University of Copenhagen","In a society dominated by mobile phones and still increasing media collections, Interactive Learning is slowly becoming the favoured paradigm for managing these collections. Still, however, no scaling Interactive Learning system exists on a mobile phone. In this paper, we present XQC, an Interactive Learning platform with a user interface that that fits all mobile phones, and scales to large media collections."
LSC,Myscéal 2.0: A Revised Experimental Interactive Lifelog Retrieval System for LSC'21,"Ly-Duyen Tran, Manh-Duy Nguyen, Hyowon Lee, Thanh Binh Nguyen and Cathal Gurrin","Dublin City University, Dublin City University, VNU HCM - University of Science, Insight Centre for Data Analytics, Dublin City University","Building an interactive retrieval system for lifelogging contains many challenges due to its massive multi-modal personal data besides the requirement of accuracy and rapid response for such a tool. The Lifelog Search Challenge (LSC) is the international lifelog retrieval competition that inspires researchers to develop their systems to cope with the challenges and evaluates the effectiveness of their solutions. In this paper, we upgrade our previous Myscéal and present Myscéal 2.0 system for the LSC'21 with the improved features inspired by the novice users experiments. The experiments show that a novice user achieved more than half of the expert score on average. To mitigate the gap of them, some potential enhancements were identified and integrated to the enhanced version."
LSC,LifeSeeker 3.0 : An Interactive Lifelog Search Engine for LSC'21,"Thao-Nhu Nguyen, Van-Tu Ninh, Tu-Khiem Le, Cathal Gurrin, Thanh-Binh Nguyen, Minh-Triet Tran, Annalina Caputo and Graham Healy","Dublin City University, Dublin City University, Dublin City University, University of Science & Vietnam National University Ho Chi, University of Science & Vietnam National University Ho Chi, Dublin City University, Dublin City University","In this paper we present the interactive lifelog retrieval engine developed for the LSC’21 comparative benchmarking challenge. The LifeSeeker 3.0 interactive lifelog retrieval engine is an enhanced version of our previous system participating in LSC’20 - LifeSeeker2.0. The system is developed by both Dublin City University and Ho Chi Minh University of Science. The implementation of LifeSeeker3.0 focuses on searching and filtering by text query using a weighted Bag-of-Words model with visual concept augmentation and three weighted vocabularies. The visual similarity search is improved using a bag of local convolutional features; while improving on the performance of the previous version enhancing query processing time, result displaying and browsing support."
LSC,LifeMon: A MongoDB-Based Lifelog Retrieval Prototype,Alexander Christian Faisst and Björn Þór Jónsson,"IT University of Copenhagen, IT University of Copenhagen","We present LifeMon, a new lifelog retrieval prototype targeting LSC. LifeMon is based around the MongoDB document store, which is one of a host of scalable NoSQL systems developed over the last two decades, with a semi-structured data model that seems well matched with lifelog requirements. Preliminary results indicate that the system is  efficient and that novice users can successfully use it to solve some LSC tasks."
LSC,ViRMA: Virtual Reality Multimedia Analytics at LSC 2021,Aaron Duane and Björn Þór Jónsson,"IT University of Copenhagen, IT University of Copenhagen","In this paper we describe the first iteration of the ViRMA prototype system, a novel approach to multimedia analysis in virtual reality and inspired by the M3 data model. We intend to evaluate our approach via the LSC to serve as a benchmark against other multimedia analytics systems."
LSC,Exploring Graph-querying approaches in LifeGraph,"Luca Rossetto, Matthias Baumgartner, Ralph Gasser, Lucien Heitz, Ruijie Wang and Abraham Bernstein","University of Zurich, University of Zurich, University of Basel, University of Zurich, University of Zurich, University of Zurich","The multi-modal and interrelated nature of lifelog data makes it well suited for graph-based representations. In this paper, we present the second iteration of LifeGraph, a Knowledge Graph for Lifelog Data, initially introduced during the 3rd Lifelog Search Challenge in 2020. This second iteration incorporates several lessons learned from the previous version. While the actual graph has undergone only small changes, the mechanisms by which it is traversed during querying as well as the underlying storage system which performs the traversal have been changed. The means for query formulation have also been slightly extended in capability and made more efficient and intuitive. All these changes have the aim of improving result quality and reducing query time."
LSC,Memento: A Prototype Lifelog Search Engine for LSC'21,"Naushad Alam, Yvette Graham and Cathal Gurrin","Dublin City University, Trinity College, Dublin City University","In this paper, we introduce a new lifelog retrieval system called Memento that leverages semantic representations of images and
textual queries projected into a common latent space to facilitate effective retrieval by bridging the semantic gap between complex visual scenes/events and user  nformation needs expressed as textual and faceted queries. The system, developed for the 2021 Lifelog Search Challenge also has a minimalist user interface which includes primary search, temporal search, and visual data filtering components."
LSC,LifeConcept: An Interactive Approach for Multimodal Lifelog Retrieval through Concept Recommendation,"Wei-Hong Ang, An-Zi Yen, Tai-Te Chu, Hen-Hsen Huang and Hsin-Hsi Chen","Department of Computer Science and Information Engineering, National Taiwan University, Taipei, Taiwan","The major challenge in visual lifelog retrieval is the semantic gap between textual queries and visual concepts. This paper presents our work on the Lifelog Search Challenge 2021 (LSC’21), an annual comparative benchmarking activity for comparing approaches to interactive retrieval from multimodal lifelogs. We propose an interactive lifelog search system that is aimed at accelerating the retrieval process and retrieving more precise results. In this work, we introduce several new features such as the number of people, location cluster, and object with color. Moreover, we obtain visual concepts from the images with computer vision models and propose a concept recommendation method to reduce the semantic gap. In this way, users can efficiently set up the related conditions for their requirements and search the desired images with appropriate query terms based on the suggestion."
LSC,Voxento 2.0: A Prototype Voice-controlled Interactive Search Engine for Lifelogs,"Ahmed Alateeq, Mark Roantree and Cathal Gurrin","Dublin City University, Dublin City University, Dublin City University","In this paper, we describe the second version of Voxento which is an interactive voice-based retrieval system for lifelogs that has been developed to participate in the fourth Lifelog Search Challenge LSC’21, at ACM ICMR’21. Voxento integrates a spoken interface to the lifelog dataset, which facilitates a novice user to interact with a personal lifelog using a range of vocal commands and interactions. For current version presented here, Voxento has been enhanced with new retrieval features and enhanced user interaction support. In this paper, we introduce these new features, which include dynamic result filtering, predefined interactive responses and the development of a new retrieval API. The new retrieval API evaluation showed the system detecting at least one target image in the top-50 results for 18 out of 24 LSC’20 topics."
LSC,Flexible Interactive Retrieval SysTem 2.0 for Visual Lifelog Exploration at LSC 2021,"Hoang-Phuc Trang-Trung, Thanh-Cong Le, Mai-Khiem Tran, Van-Tu Ninh, Tu-Khiem Le, Cathal Gurrin and Minh-Triet Tran","University of Science, VNU-HCM & Vietnam National University, University of Science, VNU-HCM & Vietnam National University, University of Science, VNU-HCM & Vietnam National University, Dublin City University, Dublin City University, University of Science, VNU-HCM & Vietnam National University","With a huge collection of photos and video clips, it is essential to provide an efficient and easy-to-use system for users to retrieve moments of interest with a wide variation of query types. This motivates us to develop and upgrade our flexible interactive retrieval system for visual lifelog exploration. In this paper, we briefly introduce our system (version 2.0) with the following main features. Our system supports multiple modalities for interaction and query processing, including visual query by meta-data, text query and visual information matching based on a joint embedding model, scene clustering based on visual and location information, flexible temporal event navigation, and query expansion with visual examples. With the flexibility in system architecture, we expect our system can easily integrate with new modules to enhance its functionalities."
LSC,PhotoCube at the Lifelog Search Challenge 2021,"Jihye Shin, Alexandra Waldau, Aaron Duane and Björn Þór Jónsson","IT University of Copenhagen, IT University of Copenhagen, IT University of Copenhagen, IT University of Copenhagen","The Lifelog Search Challenge (LSC) is a venue where retrieval system researchers compete in solving tasks to retrieve the correct image from a lifelog collection. At LSC 2021, we present the PhotoCube system as a new competitor. PhotoCube is an interactive media retrieval system that considers media items to exist in a hypercube in multi-dimensional metadata space. To solve tasks, users explore the contents of the hypercube by dynamically (a) applying a variety of filters and (b) projecting the hypercube to a three-dimensional cube that is visualised on screen."
LSC,Enhanced SOMHunter for Known-item Search in Lifelog Data,"Jakub Lokoc, Frantisek Mejzlik, Patrik Veselý, Miroslav Kratochvíl and Tomáš Souček","Charles University, Charles University, Charles University, Charles University","SOMHunter represents a modern light-weight framework for known-item search in datasets of visual data like images or videos. The framework combines an effective W2VV++ text-to-image search approach, a traditional Bayesian like model for maintenance of relevance scores influenced by positive examples, and several types of exploration and exploitation displays. With this initial setting in 2020, already the first prototype of the system turned out to be highly competitive in comparison with other state-of-the-art systems at Video Browser Showdown and Lifelog Search Challenge competitions. In this paper, we present a new version of the system further extending the list of visual data search capabilities. The new version combines localized text queries with collage queries tested at VBS 2021 in two separate systems by our team. Furthermore, the new version of SOMHunter will integrate also the new CLIP text search model recently released by OpenAI. We believe that all the extensions will improve chances to effectively initialize the search that can continue with already supported browsing capabilities."
LSC,Exquisitor at the Lifelog Search Challenge 2021: Relationships Between Semantic Classifiers,"Omar Shahbaz Khan, Björn Þór Jónsson, Aaron Duane, Jan Zahálka, Stevan Rudinac and Marcel Worring","IT University of Copenhagen, IT University of Copenhagen, IT University of Copenhagen, Czech Technical University in Prague, University of Amsterdam","Exquisitor is a scalable media exploration system based on interactive learning. To satisfy a user's information need, the system asks the user for feedback on media items and uses that feedback to interactively construct a classifier, that is in turn used to identify the next potentially relevant set of media items. To facilitate a nuanced exploration of a collection, the system offers filters to narrow the scope of exploration, search functionality for finding good examples for the classifier, and support for timeline browsing of videos or image sequences. For this year's Lifelog Search Challenge, we have enhanced Exquisitor to better support tasks with a temporal component, by adding features that allow the user to build multiple classifiers and merge the classifier results, using both traditional set operators and advanced temporal operators."
MMArt-ACM,Color-Grayscale-Pair Image Sentiment Dataset and Its Application to Sentiment-Driven Image Color Conversion,"Atsushi Takada, Xueting Wang and Toshihiko Yamasaki","The University of Tokyo, The University of Tokyo, The University of Tokyo","In this study, we focused on the fact that the color information of an image has a significant effect on the emotion recalled, and we created a dataset with discrete and continuous emotion labels for color and grayscale image pairs, which are not present in existing emotion image datasets. As a result of the analysis of the continuous-valued emotion labels, we found that the color images evoked a wider range of positive and negative emotions. We also conducted experiments to transform the color of the image to the target emotion using the emotion label and the image as input and succeeded in changing the color of the entire image according to the input emotion value. Another experiment is conducted to convert the emotion category and we confirmed that the image was generated in such a way that the evoked emotion was changed compared to the original image."
MMArt-ACM,Ketchup GAN: A New Dataset for Realistic Synthesis of Letters on Food,Gibran Benitez-Garcia and Keiji Yanai,"The University of Electro-Communications, The University of Electro-Communications","This paper introduces a new dataset for the realistic synthesis of letters on food. Specifically, the ""Ketchup GAN"" dataset consists of real-world images of egg omelettes decorated with ketchup letters. Our dataset contains sufficient size and variety to train and evaluate deep learning-based generative models. In addition, we generate a synthetic ketchup-free set, which enables us to train paired-based generative adversarial networks (GAN). The ketchup GAN dataset comprises more than two thousand images of omelette dishes collected from Twitter. Automatically generated segmentation masks of egg and ketchup are also provided as part of the dataset. Thus, we can evaluate generative models based on segmentation inputs as well. With our dataset, two state-of-the-art GAN models (Pix2Pix and SPADE) are reviewed on photorealistic ketchup letter synthesis. We finally present an automatic application of omelette decoration with ketchup text input from users. The dataset and models will be available on Github upon the acceptance of the paper."
MMArt-ACM,Estimating Groups of Featured Characters in Comics with Sequence of Characters’ Appearance,"Kodai Imaizumi, Ryosuke Yamanishi, Yoko Nishihara and Takahiro Ozawa","Ritsumeikan University, Kansai University, Manga Artist Ume, Ritsumeikan University, Manga Artist Ume","This paper proposes a method to estimate a group of featured characters during an arbitrary period in comics. Comics have so many aspects of attractiveness such as illustrations and quotes. The storyline, which is one of the attractive of comics, enables us to enjoy the dramas in comics.  In comics, the story is driven by characters’ activities represented in multimedia forms: emotion, speech, and some other actions.  This paper, as a first step to recognize the storyline, tackles the estimation of a group of featured characters in a given period.  To compute the storyline of comics with the facility, the proposed method uses a sequence of characters’ appearance for each page.  The experiment showed that the proposed method outperformed the comparative methods in estimating groups of featured characters with a 0.82 F-value on average while comparative methods showed 0.67 and 0.48.  The results showed that sequences of characters’ appearance, which are relatively easy to obtain, were sufficient to catch the brief storyline i.e. featured characters in a story."
MMPT,"Be Specific, Be Clear: Bridging Machine and Human Captions by Scene-Guided Transformer","Yupan Huang, Zhaoyang Zeng and Yutong Lu","Sun Yat-sen University, Sun Yat-sen University, Sun Yat-sen University","Automatically generating natural language descriptions for images, i.e., image captioning, is one of the primary goals for image understanding. The recent success of deep neural networks in image captioning has been accompanied by region-based bottom-up-attention features. Region-based features are representative of the contents of local regions while lacking an overall understanding of images, which is critical to more specific and clear language expression. Visual scene perception can facilitate overall understanding and provide prior knowledge to generate specific and clear captions of objects, object relations, and overall image scenes. In this paper, we propose a Scene-Guided Transformer (SG-Transformer) model that leverages the scene-level global context to generate more specific and descriptive image captions. SG-Transformer adopts an encoder-decoder architecture. The encoder aggregates global scene context as external knowledge with object region-based features in attention learning to facilitate object relation reasoning. It also incorporates high-level auxiliary scene-guided tasks towards more specific visual representation learning. Then the decoder integrates both object-level and scene-level information refined by the encoder for an overall image perception. Extensive experiments on MSCOCO and Flickr30k benchmarks show the superiority and generality of SG-Transformer. In particular, a single model of SG-Transformer achieves 130.4% (c40) CIDEr-D score on MSCOCO online evaluation and 75.5% CIDEr-D score on the Flickr30k test set. Besides, the proposed scene-guided approach can enrich both object-level and scene graph visual representations in the encoder and generalize to both RNN- and Transformer-based architectures in the decoder."
MMPT,Language-Conditioned Region Proposal and Retrieval Network for Referring Expression Comprehension,"Yanwei Xie, Daqing Liu, Xuejin Chen and Zheng-Jun Zha","University of Science and Technology of China, University of Science and Technology of China, University of Science and Technology of China, University of Science and Technology of China","Referring expression comprehension (REC) is a multi-modal task that aims to localize target regions in images according to language descriptions. Existing methods can be concluded into two categories, proposal-based methods and proposal-free methods. Proposal-based methods first detect all candidate objects in the image and then retrieve the target among those objects based on the language description, while proposal-free methods directly locate the region based on the language without any region proposals. However, the proposal-based methods suffer from separate region proposal networks that actually do not suit this task well, and the proposal-free methods are not able to perform fine-grained visual-language alignments to yield higher precision. To overcome the above drawbacks, we propose a language-conditioned region proposal and retrieval network that first detects those regions only related to the language and then retrieves the target region by compositional reasoning on the language. Specifically, the proposed network consists of a language-conditioned region proposal network (LC-RPN) to detect those language-related regions, and a language-conditioned region retrieval network (LC-RRN) to perform region retrieval with a full understanding of the language. A pre-training mechanism is proposed to teach our model knowledge about language decomposing and vision-language alignment. Experimental results demonstrate that our proposed method achieves leading performance with high inference speed on RefCOCO, RefCOCO+, and RefCOCOg benchmarks."
MMPT,Residual Recurrent CRNN for End-to-End Optical Music Recognition on Monophonic Scores,"Aozhi Liu, Lipei Zhang, Yaqi Mei, Baoqiang Han, Zifeng Cai, Zhaohua Zhu and Jing Xiao","Ping An Technology(Shenzhen) Co.,Ltd, University of London, Ping An Technology(Shenzhen) Co.,Ltd, Ping An Technology(Shenzhen) Co.,Ltd, Ping An Technology(Shenzhen) Co.,Ltd","One of the challenges of the Optical Music Recognition task is to transcript the symbols of the camera-captured images into digital music notations. Previous end-to-end model which was developed as a Convolutional Recurrent Neural Network does not explore sufficient contextual information from full scales and there is still a large room for improvement. We propose an innovative  framework that combines a block of Residual Recurrent Convolutional Neural Network with a recurrent Encoder-Decoder network to map a sequence of monophonic music symbols corresponding to the notations present in the image. The Residual Recurrent Convolutional block can improve the ability of the model to enrich the context information. The experiment results are benchmarked against a publicly available dataset called CAMERA-PRIMUS, which demonstrates that our approach surpass the state-of-the-art end-to-end method using Convolutional Recurrent Neural Network."
MMPT,Style-Guided Image-to-Image Translation for Multiple Domains,"Tingting Li, Huan Zhao, Song Wang and Jing Huang","School of Information Science and Engineering, Hunan University, School of Information Science and Engineering, Hunan University, School of Information Science and Engineering, Hunan University","The cross-domain image translation has drawn more and more attention. It aims to translate images from a source domain into target domains, such that images can appears with multiple styles. The most popular approaches are using encoders to extract style features from source-domain, and then pushing them into a generator to produce new images. However, these methods usually only suit for two domains translation, and present low-diversity in multiple domains since the extracted features are rough used as input for the generator, instead of making full use of them. In this paper, we design a novel loss function, style-guided diversity loss, which utilizes the extracted style features to encourage our model exploring the image space and discovering diverse images. It is proved theoretically that the proposed loss is better than the diversity sensitive loss in the state-of-the-art approaches. In addition, qualitative and quantitative experiments demonstrate the superiority of the proposed approach against several state-of-the-art approaches in terms of the quality and the diversity of translated images."
MMPT,A Fair and Comprehensive Comparison of Multimodal Tweet Sentiment Analysis Methods,"Gullal Singh Cheema, Sherzod Hakimov, Eric Müller-Budack and Ralph Ewerth","TIB - Leibniz Information Center for Science and Technology, TIB - Leibniz Information Center for Science and Technology, TIB - Leibniz Information Center for Science and Technology, TIB - Leibniz Information Center for Science and Technology & Leibniz University Hannover","Opinion and sentiment analysis is a vital task to characterize subjective information in social media posts. In this paper, we present a comprehensive experimental evaluation and comparison with six state-of-the-art methods, from which we have re-implemented one of them. In addition, we investigate different textual and visual feature embeddings that cover different aspects of the content, as well as the recently introduced multimodal CLIP embeddings. Experimental results are presented for two different publicly available benchmark datasets of tweets and corresponding images. In contrast to the evaluation methodology of previous work, we introduce a reproducible and fair evaluation scheme to make results comparable. Finally, we conduct an error analysis to outline the limitations of the methods and possibilities for the future work."
MMPT,Unsupervised Training Data Generation of Handwritten Formulas using Generative Adversarial Networks with Self-Attention,"Matthias Springstein, Eric Müller-Budack and Ralph Ewerth","TIB – Leibniz Information Centre for Science and Technology, TIB -  Leibniz Information Centre for Science and Technology & Leibniz University Hannover","The recognition of handwritten mathematical expressions in images and video frames is a difficult and unsolved problem yet. Deep convectional neural networks are basically a promising approach, but typically require a large amount of labeled training data. However, such a large training dataset does not exist for the task of handwritten formula recognition. In this paper, we introduce a system that creates a large set of synthesized training examples of mathematical expressions which are derived from \latex documents. For this purpose, we propose a novel attention-based generative adversarial network to translate rendered equations to handwritten formulas. The datasets generated by this approach contain hundreds of thousands of formulas, making it ideal for pretraining or the design of more complex models. We evaluate our synthesized dataset and the recognition approach on the CROHME 2014 benchmark dataset. Experimental results demonstrate the feasibility of the approach. "
Tutorial,Cross-modal Retrieval and Its Applications in Multimedia Advertising,Tan Yu and Jingjing Meng,"Baidu Research, SUNY Buffalo","Cross-modal retrieval bridges the modal gap between text and vision. It is a critical task in many multimedia applications. In the past decade, we have witnessed significant progress in cross-modal retrieval. The architecture of the mainstream method for cross-modal retrieval has evolved from the joint-embedding structure to the attention-based structure. More recently, many cross-modal BERT methods emerge. Benefited from exploiting cross-modal attention and being pre-trained on the large-scale dataset, they have achieved state-of-the-art performance in cross-modal retrieval. Meanwhile, the significant advances achieved in the cross-modal retrieval model provide a more reliable solution to multimedia advertising and boost the emergence of the multimedia advertising market. In this tutorial, we will introduce the basics for cross-modal image/video retrieval as well as the application of cross-modal retrieval in multimedia advertising."
Keynote 3,"Styles, Trends, and Influences from Large-Scale In-the-Wild Fashion Photos",Kristen Grauman,University of Texas at Austin,"The fashion domain is a magnet for computer vision. New vision problems are emerging in step with the fashion industry's rapid evolution towards an online, social, and personalized business. Style models, trend forecasting, and recommendation all require visual understanding with rich detail and subtlety. Importantly, not only can this visual understanding benefit individual users, but when analyzed across large-scale multi-modal data, it also can reveal how cultural factors and world events dynamically influence what people around the world wear. I will present our work developing computer vision methods for fashion. To begin, we explore how to discover styles from Web photos, so as to optimize mix-and-match wardrobes, suggest minimal edits to make an outfit more fashionable, and recommend clothing that flatters diverse human body shapes. Next, turning to the world stage, we investigate fashion forecasting and influence. Learned directly from photos, our models forecast what styles will be popular in the future, while accounting for how trends propagate in space and time across 44 major world cities. Finally, building on this notion of fashion influence, we quantify which cultural factors (as captured by millions of news articles) most affect the clothes people choose to wear across a century of vintage photos."
Keynote 1,How 5G and AI are Powering Our Intelligent Future,Allen Lu,MediaTek Inc.,"With the new generation of 5G + AI technology, the post-epidemic era is spawning the next wave of connected edge-computing SoC revolution. Essentially, the success of AI relies on big learning data and powerful computing capabilities. Behind the good user experience of AI benefits, it is often accompanied by personal privacy and security issues. To satisfy the better user experience such as personal privacy, real-time response and ubiquitous availability, the market strongly demands connected edge devices. For smartphone devices, mobile AI is evolving from face recognition and object detection to image enhancement. In terms of high-end TV, AI functions are also evolving from scene detection and image segmentation to pixel-level super resolution. Such demand for higher computing power and resources is accelerating the innovation of connected smart devices. From daily life at home and workplace to the transportation process, practical intelligence has become an irreversible trend.

In practical AI applications, deep neural network operations need to integrate multiple computing processes at the same time, such as image processing, 3D graphics, and wireless transmission. The demand brings higher and higher challenges to chip design, especially in complex multiplexing, heat generated by each computing unit, and finite memory bandwidth. This presentation will discuss the new applications and opportunities brought by breakthroughs 5G + AI technology, as well as new challenges to chip design."
Keynote 2,Faces & Emotional AI,Maja Pantic,Imperial College London,"This talk is about emotional AI, about machine learning and computer vision methods developed for various human-centric AI applications, and about the face analysis technology in general."
